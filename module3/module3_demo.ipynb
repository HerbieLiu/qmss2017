{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - R Crosstalk (cont.)\n",
    "## Jupyter Magic\n",
    "Jupyter R magic can be used to make calls to R from Jupyter Notebooks and to return outputs from R functions into the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -o x\n",
    "x = seq(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Follow-ups\n",
    "Pandas can read tabulated data directly from URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "## Read data from URL.\n",
    "url = 'https://raw.githubusercontent.com/szorowi1/qmss2017/master/module3/random/random.csv'\n",
    "data = read_csv(url)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can be set as new Pandas indices using the **set_index** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.set_index('y')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be especially useful for fast indexing of specific outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc['hotel'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dplyr in Python: *dplython* library\n",
    "To install, open the Terminal and type:\n",
    "```\n",
    "pip install dplython\n",
    "```\n",
    "For full documentation, please see [here](https://github.com/dodger487/dplython)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dplython import DplyFrame, X, select, sift, head, mutate, group_by, summarize\n",
    "\n",
    "## Convert DataFrame to DplyFrame\n",
    "data_dply = DplyFrame(data.reset_index())\n",
    "\n",
    "## Example with sift, select, and head.\n",
    "data_dply >> sift( X.x01 < -5 ) >> select( X.x02, X.x03, X.x04 ) >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Example with mutate, group_by, and summarize.\n",
    "(data_dply >> \n",
    "  mutate(x01_bin=X.x01.round()) >> \n",
    "  group_by(X.y, X.x01_bin) >> \n",
    "  summarize(x02_mean=X.x02.mean())) >> head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dplyr in Python: *pandas-ply* library\n",
    "To install, open the Terminal and type:\n",
    "```\n",
    "pip install pandas-ply\n",
    "```\n",
    "For full documentation, please see [here](https://pythonhosted.org/pandas-ply/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_ply import install_ply, X, sym_call\n",
    "\n",
    "## After calling install_ply, all pandas objects have \n",
    "## pandas-plyâ€˜s methods attached.\n",
    "install_ply(pd)\n",
    "\n",
    "## Piping example.\n",
    "(data\n",
    " .reset_index()\n",
    " .groupby('y')\n",
    " .ply_select(\n",
    "    arr = X.x01.mean(),\n",
    "    dep = X.x02.mean())\n",
    " .ply_where(X.arr < -5, X.dep > 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dplyr in Python: *rpy2* library\n",
    "\n",
    "For more information, please see [here](http://blog.yhat.com/posts/rpy2-combing-the-power-of-r-and-python.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import functions from R, use **robjects.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R object with classes: ('integer',) mapped to:\n",
       "<IntVector - Python:0x10b478ac8 / R:0x7f9b6bb9d898>\n",
       "[       1,        2,        3,        4,        5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rpy2.robjects as robjects\n",
    "\n",
    "seq = robjects.r('seq')\n",
    "seq(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import whole libraries, use **importr**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "lme4 = importr('lme4')\n",
    "lme4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Tutorial\n",
    "\n",
    "The following will be a brief overview of machine learning with python. For more complete treatments, please see:\n",
    "* [Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)\n",
    "\n",
    "## Overview\n",
    "Scikit-Learn, or sklearn, is the foremost machine learning library in Python. It contains a number of state-of-the-art machine learning algorithms, as well as comprehensive [documentation](http://scikit-learn.org/stable/index.html). It is impressively fast, and the functions are constructed in such a way so as to involve the least amount of code possible for the user. Moreover, Scikit-Learn is designed for maximal compatibility with Pandas, making the training and testing of data very easy. \n",
    "\n",
    "At the highest level of description, all algorithms in scikit-learn are implemented as pythonic classes. All algorithms must first be initialzied (i.e. must initialize an instance of the class) with specified parameters. All algorithmic intances have a \"fit\" attribute for applying the algorithm to data. NumPy arrays, sparse data, and Pandas data structures are all accepted as inputs. \n",
    "\n",
    "To practice with Scikit-Learn, we will be applying some machine learning techniques to both \"clean\" and \"messy\" datasets. The purpose for using multiple datasets is to highlight the validity of the code with clean data (by reproducing classic clustering/classification/regression results), and demonstrate more complex use cases with noisier data. We introduce the datasets below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0dd7a3b8a990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import get_dummies\n",
    "\n",
    "## Make dummy coded variables.\n",
    "get_dummies(data.index).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digitizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define bins.\n",
    "bins = [-9,-7,-5,-3,-1]\n",
    "\n",
    "x = data.x01\n",
    "xd = np.digitize(x, bins)\n",
    "\n",
    "for a,b in np.c_[x,xd][::20]:\n",
    "    print('%0.3f\\t%0.0f' %(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "Scikit-Learn includes several built-in preprocessing algorithms for transforming/recoding data:\n",
    "* **StandardScaler:** scales a variable such that it is mean = 0, sd = 1\n",
    "* **RobustScaler:** scales a variable using medians and quartiles (i.e. 25% of data falls beneath 1st quartile)\n",
    "* **MinMaxScaler:** scales a variable between a specified min and max (e.g. [0,1])\n",
    "* **Normalizer:** scales each data point such that the feature vector has a euclidean length of one\n",
    "\n",
    "We will test two of these now, applying the RobustScaler to all continous data and the MinMaxScaler to all nominal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Apply standard-score (z-score) transform to data. \n",
    "data.loc[:,:] = StandardScaler().fit_transform(data.loc[:,:])\n",
    "\n",
    "data.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook', font_scale=2)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(data.corr(), vmin=-1, vmax=1, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "variance_inflation_factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Compute VIF.\n",
    "VIF = [variance_inflation_factor(data.as_matrix(), n) for n in range(data.shape[-1])]\n",
    "\n",
    "## Plot.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "color, = sns.color_palette(n_colors=1)\n",
    "ax = sns.barplot(np.arange(data.shape[-1]), VIF, color=color);\n",
    "ax.set(xticklabels=data.columns,  ylabel='VIF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drop column x09.\n",
    "data = data.drop('x09', axis=1)\n",
    "\n",
    "## Compute VIF.\n",
    "VIF = [variance_inflation_factor(data.as_matrix(), n) for n in range(data.shape[-1])]\n",
    "\n",
    "## Plot.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "color, = sns.color_palette(n_colors=1)\n",
    "ax = sns.barplot(np.arange(data.shape[-1]), VIF, color=color);\n",
    "ax.set(xticklabels=data.columns, ylim=(0,15), ylabel='VIF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Scikit-Learn also includes a number of featuers for the automatic reduction of features to those most explanatorily useful. These include:\n",
    "* **Univariate approches:** selection of features with statistically significant relationships to the target. These approaches only consider each feature individually. Consequently a feature will be discarded if it is only informative when combined with another feature. \n",
    "* **Model-based approaches:** use a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones. The supervised model that is used for feature selection doesnâ€™t need to be the same model that is used for the final supervised modeling.\n",
    "* **Iterative selection:** a series of models is built, with varying numbers of features. There are two basic methods: starting with no features and adding features one by one, until some stopping criterion is reached, or starting with all features and removing features one by one, until some stopping criterion is reached. \n",
    "\n",
    "Model-based and iterative selection methods are more complicated, powerful approaches. For now, we will demonstrate feature selection with the simpler univariate approaches. There are several functions to choose from including **SelectKBest**, **SelectPercentile**, **SelectFdr**, and **SelectFwe**. As the names imply, the first two functions select the K-best and N%-best features, whereas the latter two select features that are statistically significant after multiple comparisons corrections.\n",
    "\n",
    "With 271 observations and 159 features, the phenotype dataset could stand to use a little more pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, SelectFdr, SelectFwe\n",
    "\n",
    "## Initialize feature selection classes.\n",
    "fwe = SelectFwe(alpha=0.05)\n",
    "\n",
    "## Fit to data.\n",
    "fit = fwe.fit(data, data.index)\n",
    "\n",
    "## Find surviving columns.\n",
    "fwe_cols = fit.get_support()\n",
    "fwe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn attributes can be chained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwe_cols = SelectFwe(alpha=0.05).fit(data,data.index).get_support()\n",
    "fwe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce data to surviving columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[data.columns[fwe_cols]]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Initialize with desired number of components.\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "## Fit to data.\n",
    "fit = pca.fit(data)\n",
    "print(fit.explained_variance_ratio_)\n",
    "\n",
    "## Transform.\n",
    "data_2d = fit.transform(data)\n",
    "print(data_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "## Fit into DataFrame.\n",
    "data_2d = DataFrame(fit.transform(data), columns=('PCA1','PCA2'), index=data.index)\n",
    "\n",
    "## Plot.\n",
    "y_types = data_2d.index.unique()\n",
    "colors = sns.color_palette(n_colors=len(y_types))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "for y, color in zip(y_types, colors):\n",
    "    ax.scatter(data_2d.loc[y, 'PCA1'], data_2d.loc[y, 'PCA2'], color=color, label=y)\n",
    "ax.set(xlabel='PCA1', ylabel='PCA2')\n",
    "ax.legend(loc=7, bbox_to_anchor=(1.25,0.5), handletextpad=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Sets\n",
    "Scikit-learn contains a function, **train_test_split**, that shuffles the dataset and splits it for you. This is perhaps the most essential function in all of Scikit-Learn. Fortunately it handles Pandas arrays making it easily compatible with any data that can be read into Python via Pandas. We demonstrate this function below on the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Separate out precictors and outcomes.\n",
    "X = data.as_matrix()\n",
    "y = data.index\n",
    "\n",
    "## Split our dataset into four variables using train_test_split.\n",
    "## We hold out 20% of the data for prediction. All but the last\n",
    "## column are predictors.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Original data shape: (%s, %s)' %data.shape)\n",
    "print('Train data shape: (%s, %s)' %X_train.shape)\n",
    "print('Test data shape: (%s, %s)' %X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "## Initialize SVC with desired parameteres.\n",
    "svc = LinearSVC(penalty='l2', C=1.0, random_state=0)\n",
    "\n",
    "## Fit to data.\n",
    "fit = svc.fit(X_train, y_train)\n",
    "\n",
    "## Compute predictive accuracy to test data.\n",
    "print('Training performance: %0.3f' %fit.score(X_train, y_train))\n",
    "print('Test performance: %0.3f' %fit.score(X_test, y_test))\n",
    "\n",
    "## Show predictions.\n",
    "predictions = fit.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confmat = DataFrame(confusion_matrix(y_test, predictions), columns=np.unique(y_test), \n",
    "                    index=np.unique(y_test))\n",
    "confmat = confmat.apply(lambda x: x / x.sum(), 1)\n",
    "\n",
    "confmat.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "One downside to train_test_split is that it splits the data only once. This can be problematic as that one model fit may be unrepresentative of the underlying true model fit as a result of the random allocation of data to the train/test sets. Thus, it is better to test models on multiple sets of the data. Fortunately, Scikit-Learn has many [built-in functions](http://scikit-learn.org/stable/modules/cross_validation.html) for sampling and resampling train/test sets:\n",
    "* **K-fold:** divides all the samples in k groups of samples, called folds of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test.\n",
    "* **Leave One Out (LOO):** each learning set is created by taking all the samples except one, the test set being the sample left out. This cross-validation procedure does not waste much data as only one sample is removed from the training set.\n",
    "* **Leave P Out (LPO):** very similar to LeaveOneOut as it creates all the possible training/test sets by removing p samples from the complete set.\n",
    "* **Shuffle & Split:** generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.\n",
    "* **Stratified k-fold:** variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "* **Stratified Shuffle & Split:**  variation of ShuffleSplit, which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set.\n",
    "\n",
    "We demonstrate two of these functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "StratifiedShuffleSplit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_ix, test_ix in sss.split(X, y):\n",
    "    print(train_ix.shape, test_ix.shape)\n",
    "    \n",
    "print(train_ix[:10])\n",
    "print(test_ix[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define train sizes.\n",
    "train_sizes = np.linspace(0.2,0.8,13)\n",
    "n_train_sizes = train_sizes.shape[0]\n",
    "n_splits = 10\n",
    "test_size = 0.2\n",
    "\n",
    "## Preallocate space for performance data.\n",
    "performance = np.zeros((n_train_sizes,n_splits,3))\n",
    "\n",
    "## Iteratively compute scores.\n",
    "for i, train_size in enumerate(train_sizes):\n",
    "    \n",
    "    ## Initialize cross-validation tool.\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, train_size=train_size, \n",
    "                                 test_size=test_size, random_state=0)\n",
    "    \n",
    "    for j, indices in enumerate(sss.split(X,y)):\n",
    "        \n",
    "        ## Extract indices.\n",
    "        train_ix, test_ix = indices\n",
    "        \n",
    "        ## Fit SVC.\n",
    "        fit = svc.fit(X[train_ix], y[train_ix])\n",
    "        \n",
    "        ## Compute performance scores.\n",
    "        train_score = fit.score(X[train_ix], y[train_ix])\n",
    "        test_score = fit.score(X[test_ix], y[test_ix])\n",
    "        \n",
    "        ## Store information.\n",
    "        performance[i,j,0] = train_size\n",
    "        performance[i,j,1] = train_score\n",
    "        performance[i,j,2] = test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert to DataFrame.\n",
    "performance = DataFrame(performance.reshape(n_train_sizes*n_splits,3), \n",
    "                        columns=('Train Size','Train', 'Test'))\n",
    "\n",
    "## Melt into longlist.\n",
    "performance = performance.melt(id_vars='Train Size', var_name='Model', value_name='Score')\n",
    "\n",
    "## Plot.\n",
    "sns.factorplot('Train Size', 'Score', 'Model', data=performance, aspect=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define parameters.\n",
    "C_params = np.power( 10., [-2, -1, 0, 1, 2] )\n",
    "n_c_params = C_params.shape[0]\n",
    "kfold = 10\n",
    "\n",
    "## Preallocate space for performance data.\n",
    "performance = np.zeros((n_c_params,kfold,2))\n",
    "\n",
    "for i, C in enumerate(C_params):\n",
    "    \n",
    "    ## Initialize SVC.\n",
    "    svc = LinearSVC(penalty='l2', C=C, random_state=0)\n",
    "    \n",
    "    ## Perform cross-validation.\n",
    "    scores = cross_val_score(svc, X, y, cv=kfold)\n",
    "    \n",
    "    ## Store.\n",
    "    performance[i,:,0] = C\n",
    "    performance[i,:,1] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert to DataFrame.\n",
    "performance = DataFrame(performance.reshape(n_c_params*kfold,2), columns=('C','Score'))\n",
    "\n",
    "## Plot.\n",
    "sns.swarmplot('C','Score',data=performance);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
