{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: Wine Quality\n",
    "This dataset is comprised of many measurements of different Portuguese wines (downloaded from [here](https://archive.ics.uci.edu/ml/datasets/Wine+Quality), published [here](http://www.sciencedirect.com/science/article/pii/S0167923609001377)). Specifically, 11 attributes were measured for several hundred red and white wines (e.g. acidity, sugar, sulfar, pH), and each of their qualities were rated from 0 to 10. \n",
    "\n",
    "In these exercises, we will use a variety of unsupervised and supervised learning techniques to classify and predict different wine attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load and Preprocessing the Data\n",
    "### Load and concatenate the two wine datasets. \n",
    "**Hint:** Remember to add a new column denoating the red and white wines. Also, the datasets are semi-colon-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize thie distribution of each of the 12 attributes with histograms. \n",
    "This can be accomplished in several ways. Two such ways are:\n",
    "1. Initialize multiple plots through Matplotlib and use displot, as shown [here](http://seaborn.pydata.org/examples/distplot_options.html). Multiple for loops will be necessary.\n",
    "2. Melt the dataframe preserving only the color attribute. Then use FacetGrid as shown [here](http://seaborn.pydata.org/examples/faceted_histogram.html). You will want to turn off sharex/sharey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib + Distplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn only: Facetgrid + Displot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the plotting method, two trends should be apparent: \n",
    "1. Certain attributes strongly differentiate red vs. white lines, including the acidity, sulfur, and pH variables.\n",
    "2. Several of the variables are variables are clearly non-normally distributed, especially citric acid and alcohol. \n",
    "\n",
    "The next step will be to normalize the variables to some constant scale for ease of fitting various machine learning models.\n",
    "\n",
    "### Normalize the wine attributes (not including Quality) with the RobustScaler from Scitkit-Learn.\n",
    "Look back at the *Data Transformations with Scikit-Learn* section of Module 3 notes for a reminder if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the lmplot function from Seaborn, plot the 11 attributes against Quality, split by color.\n",
    "The final plot should have the attributes on the x-axis and quality on the y-axis. \n",
    "\n",
    "**Hint:** The DataFrame will need to be melted first, preserving quality and color. See [here](https://seaborn.pydata.org/examples/anscombes_quartet.html), [here](https://seaborn.pydata.org/examples/multiple_regression.html), and the Diabetes Dataset section of the Module 3 notes for inspiration. Try messing with col_wrap, sharex, and sharey, y_jitter for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it should now be clear, several of the variables seem to show a relationship to the Quality score. For example, higher volatile acidities seem to predict lower quality scores. In contrast, increased alcohol content seems to predict higher quality scores.\n",
    "\n",
    "### Using Statsmodels, compute and visualize the variance inflation factor of the 11 attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though density seems to be a little high (VIF ~= 5.55), we will leave it be. Otherwise our variables seem to be largely non-collinear meaning that we should have no trouble fitting linear models.\n",
    "\n",
    "# Section 2: Unsupervised Learning\n",
    "In this section, we will attempt to cluster the two wine types (red & color) without any training. \n",
    "\n",
    "### Perform principal components analysis on the 11 wine attributes, extracting the first two principle components. Print the explained variance of the first two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the scatter function from Matplotlib or the kdeplot from Seaborn, make a scatterplot of the first two principal components split by wine color.\n",
    "See [here](https://seaborn.pydata.org/examples/multiple_joint_kde.html) for a tutorial using kdeplot.\n",
    "\n",
    "In either case, you will need to index into your PCA-reduced data twice: once per wine color. For nicer plotting colors, try using the Seaborn [color palette tools](http://seaborn.pydata.org/tutorial/color_palettes.html). Try to set a legend as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Matplotlib and scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Seaborn and kdeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty clear from PCA that even two components are capable of separating out the two wine types. \n",
    "\n",
    "### Perform k-means clustering on the 11 wine attributes. Compute the accuracy score of the k-means fit.\n",
    "To do this, we will need to binarize the wine color variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to PCA, K-means clustering accurately predicts wine type.\n",
    "\n",
    "### Make a denodrogram using the Ward agglomerative clustering method.\n",
    "**NOTE:** Caution, this exercise may be a little slow. Feel free to skip if running short on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Supervised Learning\n",
    "In this section, we will employed supervised learning approaches to both predict wine color and to predict quality score. \n",
    "\n",
    "### While using the Scikit-Learn logistic regression method to predict wine color, demonstrate and visualize the effect of different training size sets on test performance.\n",
    "Specifically, test training sets in the range of [0.25,0.75,0.10] and test size = 0.25. Use either penalty function (l1 or l2).\n",
    "\n",
    "For visualization, try out Seaborn [boxplots](https://seaborn.pydata.org/examples/horizontal_boxplot.html), [swarmplots](https://seaborn.pydata.org/examples/scatterplot_categorical.html), or [violinplots](https://seaborn.pydata.org/examples/simple_violinplots.html). In any case, it will be easiest for you to store the scores in a new DataFrame. Take inspiration from the Module 3 Notes to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Logistic Regression with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble scores into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn Boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn Swarmplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn Violinplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a logistic regression model to 75/25 split, compute scores, and plot the coefficients sorted by magnitude.\n",
    "Be sure to include the (sorted) column names as xticklabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning now to predicting the quality score of wines, compare the performance of OLS linear regression against ridge regression for a  sampling of training set sizes.\n",
    "Specifically, test training sets in the range of [0.25,0.75,0.10] and test size = 0.25. Visualize using lmplot, splitting between the two regression model types.\n",
    "\n",
    "**Hint:** Try preallocating a scores matrix to make the post-processing of the scores into DataFrame easier. You will need to make an empty 3-dimensional matrix of size [n_train_sizes, n_splits, 3]. The 3rd dimension will track: (test_size, OLS, Lasso).\n",
    "\n",
    "See [here](https://seaborn.pydata.org/examples/multiple_regression.html) for visualization help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models show similar performance with increase generalization with increasing training set size and an upwards ceiling of R2 = 0.300. Let's see if we can improve model fit with other models.\n",
    "### Fit a random forest regressor model to the 11 attributes to predict quality.\n",
    "Using the same multiple 80/20 splits, evaluate the effect of forest size on predictive ability. Visualize with the swarmplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the performance of the random forest method, fit an 80/20 split, compute socre, and show most important coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
