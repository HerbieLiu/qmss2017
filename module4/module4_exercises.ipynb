{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: Amazon Food Reviews\n",
    "This exercise will involve analyzing a dataset of user reviews of food items from Amazon.com. Originally, over 568,000 reviews were collected between 1999 and 2012. For the sake of computational ease, the current dataset has been decimated to 56,846 observations. The dataset is split between two files: *scores.txt*, which documents the user rating given to an item, and *reviews.txt*, which contains the text-based review left by the user. In both files, observations are split by line. In this exercise, we will perform simplified sentiment analysis, exploring which tokens are predictive of a low rating (1 Star) and a high rating (5 stars). \n",
    "\n",
    "# Section 1: Text Processing\n",
    "In this section, you will write a text processing pipeline will iteratively read in the Amazon reviews, line-by-line, from *reviews.txt*, perform a series of cleaning steps, and save them into a new file, *reviews_tokenized.txt*. There is no one right text processing pipeline, and decisions to include or exclude certain processing steps should be made based on the corpus. Construct your own text processing pipeline below, and experiment with including certain features (e.g. piped-stemming via POS-tagging, spellcheck and correcting spelling, inclusion/exclusion of numbers). Document your decision making, and see the Module 4 Solutions notebook for guidance/inspiration.\n",
    "\n",
    "**NOTE:** Depending on the complexity of your pipeline, writing to file may take a while. Sam's pipeline took approximately **10 minutes** to complete. For the impatient, feel free to use the pre-tokenized file and continue with the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Sentiment Analysis\n",
    "In this section, we will take the outputs of your text processing pipeline and attempt to learn which words are most predictive of low and high Amazon ratings.\n",
    "\n",
    "### Using CountVectorizer, perform frequency counts of the tokens of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the most frequent words using WordCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the distribution of token frequency/ratio in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recount tokens removing singletons and reduce to tokens only appearing 5% or fewer documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the feature-frequency matrix using TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the the multinomial Naive Bayes classifier and 5-fold cross-validation, find which alpha level seems to generate the best scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the determined alpha level, fit a 80/20 split to the data. Find which tokens are most representative of each Amazon rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "99px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
