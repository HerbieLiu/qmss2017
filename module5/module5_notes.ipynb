{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Authentication: User vs Application\n",
    "For most APIs, it is important to note the distinction between **authentication types.** There are two types of [authentication calls](https://twython.readthedocs.io/en/latest/usage/starting_out.html) that can be made and they influence the downstream functionality of the module. They are:\n",
    "* **User Authentication:** user authenticated calls for direct interactions with users (e.g. tweeting, following people, sending DMs, etc.)\n",
    "* **Application Authentication:** application authenticated calls for making read-only calls to Twitter (e.g. searching, reading a public users timeline)\n",
    "\n",
    "Below we will only be handling application authentication and subsequent calls, as this is likely the use case for data analysis (e.g. gather data, store offline, analyze offline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook API\n",
    "To use Facebook's API, you will first need to [register](https://developers.facebook.com/) for an application with Facebook. Once there, create and new app and fill out the requisite details (e.g. name, description, website, etc.). \n",
    "\n",
    "After registering, you will need your **App ID** and **App Secret**, which will be used to authorize your API calls. Be sure to save in a protected manner. \n",
    "\n",
    "## Searching Facebook\n",
    "The primary function with an OAuth2-authenticated Twitter instance is the search function, which returns data related to the search from the Facebook graph structure. Before detailing the search function, it's worth first going over the Facebook graph API structure.\n",
    "\n",
    "### Graph API\n",
    "From the Facebook devs, \"the Graph API is the primary way to get data out of, and put data into, Facebook's platform.\" Though the graph structure has been thoroughly described [elsewhere](https://developers.facebook.com/docs/graph-api/overview), the concise explanation is that the Graph API is structured akin to a social network in that it is comprised of the following:\n",
    "* **Nodes:** top-level classes, such as Users, Photos, Pages, and Comments\n",
    "* **Edges:** relationships between those classes, such as a Page's Photos, or a Photo's Comments\n",
    "* **Fields:** information about individual instances of those classes, such as a User's birthday, or a Page's title\n",
    "\n",
    "When searching the Facebook graph API, we identify search-relevant nodes and in turn store its associated data (fields) and identify related content via its connections (edges).\n",
    "\n",
    "### Search Types\n",
    "Facebook has [documented](https://developers.facebook.com/docs/graph-api/using-graph-api#search) the list of searchable node types and this list is recreated below:\n",
    "\n",
    "|Type|Description|q value|\n",
    "|----|-----------|-------|\n",
    "|user|Search for a person (if they allow their name to be searched for).|Name|\n",
    "|page|Search for a page.|Name|\n",
    "|event|Search for an event.|Name|\n",
    "|group|Search for a group.|Name|\n",
    "|place|Search for a place. You can narrow your search to a specific location and distance by adding the center parameter (with latitude and longitude) and an optional distance parameter (in meters).|Name|\n",
    "|placetopic|Returns a list of possible place Page topics and their IDs. Use with topic_filter=all parameter to get the full list.|None|\n",
    "|ad_*|A collection of different search options that can be used to find targeting options.|See [Targeting Options](https://developers.facebook.com/docs/graph-api/reference/v2.9/targeting)|\n",
    "\n",
    "### Function Call \n",
    "The [search function](https://developers.facebook.com/docs/graph-api/using-graph-api) accepts the following parameters:\n",
    "\n",
    "|Name|Required|Description|Example|\n",
    "|----|--------|-----------|-------|\n",
    "|type|Yes|The type of Facebook class to be searched|See above|\n",
    "|q|Yes|The content of the search query| Sam, python, doggos|\n",
    "|before|No|This is the cursor that points to the start of the page of data that has been returned.|ID|\n",
    "|after|No|This is the cursor that points to the end of the page of data that has been returned.|ID|\n",
    "|limit|No|This is the maximum number of objects that may be returned. A query may return fewer than the value of limit due to filtering.|15|\n",
    "|next|No|The Graph API endpoint that will return the next page of data. If not included, this is the last page of data. Due to how pagination works with visibility and privacy, it is possible that a page may be empty but contain a 'next' paging link. Stop paging when the 'next' link no longer appears.|URL|\n",
    "|previous|No|The Graph API endpoint that will return the previous page of data. If not included, this is the first page of data.|URL|\n",
    "|fields|No|Function returns only those fields specified.|id, name, picture|\n",
    "\n",
    "\n",
    "## Facebook-SDK\n",
    "From its [documentation page](https://facebook-sdk.readthedocs.io/en/latest/index.html):\n",
    ">This client library is designed to support the Facebook Graph API and the official Facebook JavaScript SDK, which is the canonical way to implement Facebook authentication. You can read more about the Graph API by accessing its official documentation.\n",
    "\n",
    "To install the latest version of Facebook-SDK (v3.0 at the time of writing), simply open a Terminal and input: \n",
    "\n",
    "\\> pip install -e git+https://github.com/mobolic/facebook-sdk.git#egg=facebook-sdk\n",
    "\n",
    "\n",
    "### Initializing Facebook Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import facebook\n",
    "import numpy as np\n",
    "\n",
    "## Load and store authentication keys.\n",
    "oath = np.load('facebook_oath.npz')\n",
    "APP_ID = oath['app_id'].astype(str).tolist()\n",
    "APP_SECRET = oath['app_secret'].astype(str).tolist()\n",
    "\n",
    "## Initialize Facebook graph object to receive ACCESS_TOKEN.\n",
    "graph = facebook.GraphAPI(version='2.7')\n",
    "\n",
    "## Store and save ACCESS_TOKEN.\n",
    "ACCESS_TOKEN = graph.get_app_access_token(APP_ID, APP_SECRET)\n",
    "\n",
    "## Re-initilize Facebook graph object with ACCESS_TOKEN to use it.\n",
    "graph = facebook.GraphAPI(access_token=ACCESS_TOKEN, version='2.9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '547581871944926', 'name': 'Python Developers'},\n",
       "  {'id': '509724922449953', 'name': 'Python Tips'},\n",
       "  {'id': '100832693336856', 'name': 'Python'},\n",
       "  {'id': '346004702246198', 'name': 'Python'},\n",
       "  {'id': '1539401562941064', 'name': 'Django - Python'}],\n",
       " 'paging': {'cursors': {'after': 'NAZDZD', 'before': 'MAZDZD'},\n",
       "  'next': 'https://graph.facebook.com/v2.9/search?access_token=1415649088531508%7Cjq-ce7ouUFY_B2ePp6dQ9ob2_zc&q=python&type=page&limit=5&after=NAZDZD'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Search for all Facebook pages mentioning python.\n",
    "results = graph.search(type='page', q='python', limit=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query results are stored in a dictionary format. At the top level, there are two keys:\n",
    "* **data:** a list of dictionaries containg the results of the search.\n",
    "* **paging:** a dictionary containing information about the query itself, including the **after**, **before**, and **next** pieces of metadata which are necessary for iterative function calls.\n",
    "\n",
    "Once a particular node in the graph API has been identified, its ID can be used to query additional features of the node, including its metadat and connections. We will only highlight a few examples below, but there is [extensive documentation](https://developers.facebook.com/docs/graph-api/reference) of the fields and connections that can be searched for a particular node type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Name: Python Developers\n",
      "Page ID: 547581871944926\n",
      "Page Link: https://www.facebook.com/PythonDevelopers/\n",
      "Page Description: Everything about python\n",
      "\n",
      "For beginners, try this book http://greenteapress.com/thinkpython2/thinkpython2.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'created_time': '2017-04-16T04:30:09+0000',\n",
       "  'id': '547581871944926_1286904234679349',\n",
       "  'story': \"Python Developers shared Code.org's post.\"},\n",
       " {'created_time': '2017-04-05T23:21:15+0000',\n",
       "  'id': '547581871944926_1277468192289620',\n",
       "  'message': 'Cool code for running python with grub bootloader :)\\nhttps://github.com/biosbits/bits'},\n",
       " {'created_time': '2017-03-17T13:13:29+0000',\n",
       "  'id': '547581871944926_1260813420621764',\n",
       "  'message': 'Firebase + Python https://github.com/thisbejim/Pyrebase'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract first page and corresponding ID.\n",
    "page = results['data'][0]\n",
    "page_id = page['id']\n",
    "\n",
    "## Extract additional metadata from page.\n",
    "metadata = graph.get_object(page_id, fields='id,name,about,link')\n",
    "print('Page Name: %s' %metadata['name'])\n",
    "print('Page ID: %s' %metadata['id'])\n",
    "print('Page Link: %s' %metadata['link'])\n",
    "print('Page Description: %s' %metadata['about'])\n",
    "\n",
    "## Extract posts from this page.\n",
    "post_results = graph.get_connections(page_id, connection_name='posts', limit=3)\n",
    "post_results['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Search Results\n",
    "The list of dictionaries can very easily be amalgamated into a single Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-16T04:30:09+0000</td>\n",
       "      <td>547581871944926_1286904234679349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Python Developers shared Code.org's post.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-05T23:21:15+0000</td>\n",
       "      <td>547581871944926_1277468192289620</td>\n",
       "      <td>Cool code for running python with grub bootloa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-17T13:13:29+0000</td>\n",
       "      <td>547581871944926_1260813420621764</td>\n",
       "      <td>Firebase + Python https://github.com/thisbejim...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time                                id  \\\n",
       "0  2017-04-16T04:30:09+0000  547581871944926_1286904234679349   \n",
       "1  2017-04-05T23:21:15+0000  547581871944926_1277468192289620   \n",
       "2  2017-03-17T13:13:29+0000  547581871944926_1260813420621764   \n",
       "\n",
       "                                             message  \\\n",
       "0                                                NaN   \n",
       "1  Cool code for running python with grub bootloa...   \n",
       "2  Firebase + Python https://github.com/thisbejim...   \n",
       "\n",
       "                                       story  \n",
       "0  Python Developers shared Code.org's post.  \n",
       "1                                        NaN  \n",
       "2                                        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "\n",
    "## Convert each status dictionary into a Pandas Series.\n",
    "posts = [Series(post) for post in post_results['data']]\n",
    "\n",
    "## Merge into single DataFrame\n",
    "df = DataFrame(posts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Due to the graph-like nature of Facebook, data organization/storage is a non-trivial design problem and shoudl reflect the ultimate analytic goals. For example, if one is interested only in text mining the comments of a Facebook page without caring about the structure of post-comment and user-comment relationships, then a flat structure may be appropriate. If more complex tree-like relations are desired, then nested data structures (e.g. nested file directories, JSON, XML, etc.) may be necesary. \n",
    "\n",
    "In any event, an example script is given below that mines comments from Barack Obama's Facebook Page. This script also obeys Facebook's [rate limit](https://developers.facebook.com/docs/graph-api/advanced/rate-limiting) of 200 requests/hr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results: Barack Obama (ID = 6815841748)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>id</th>\n",
       "      <th>like_count</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-10T23:49:31+0000</td>\n",
       "      <td>10154508876046749_10154508879381749</td>\n",
       "      <td>13088</td>\n",
       "      <td>As a young Republican. We don't always see eye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-10T23:51:55+0000</td>\n",
       "      <td>10154508876046749_10154508884101749</td>\n",
       "      <td>5875</td>\n",
       "      <td>Even tho I dont agree with everything presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-10T23:49:24+0000</td>\n",
       "      <td>10154508876046749_10154508879121749</td>\n",
       "      <td>4647</td>\n",
       "      <td>So sad, especially considering who is replacin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-10T23:50:14+0000</td>\n",
       "      <td>10154508876046749_10154508880516749</td>\n",
       "      <td>3186</td>\n",
       "      <td>I'm gonna miss you Barack Obama I wish you cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-10T23:51:03+0000</td>\n",
       "      <td>10154508876046749_10154508882091749</td>\n",
       "      <td>3124</td>\n",
       "      <td>I am so very sorry for how Congress treated yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time                                   id  like_count  \\\n",
       "0  2017-01-10T23:49:31+0000  10154508876046749_10154508879381749       13088   \n",
       "1  2017-01-10T23:51:55+0000  10154508876046749_10154508884101749        5875   \n",
       "2  2017-01-10T23:49:24+0000  10154508876046749_10154508879121749        4647   \n",
       "3  2017-01-10T23:50:14+0000  10154508876046749_10154508880516749        3186   \n",
       "4  2017-01-10T23:51:03+0000  10154508876046749_10154508882091749        3124   \n",
       "\n",
       "                                             message  \n",
       "0  As a young Republican. We don't always see eye...  \n",
       "1  Even tho I dont agree with everything presiden...  \n",
       "2  So sad, especially considering who is replacin...  \n",
       "3  I'm gonna miss you Barack Obama I wish you cou...  \n",
       "4  I am so very sorry for how Congress treated yo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import facebook\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Authentication.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load and store authentication keys.\n",
    "oath = np.load('facebook_oath.npz')\n",
    "APP_ID = oath['app_id'].astype(str).tolist()\n",
    "APP_SECRET = oath['app_secret'].astype(str).tolist()\n",
    "\n",
    "## Initialize Facebook graph object to receive ACCESS_TOKEN.\n",
    "graph = facebook.GraphAPI(version='2.7')\n",
    "\n",
    "## Store and save ACCESS_TOKEN.\n",
    "ACCESS_TOKEN = graph.get_app_access_token(APP_ID, APP_SECRET)\n",
    "\n",
    "## Re-initilize Facebook graph object with ACCESS_TOKEN to use it.\n",
    "graph = facebook.GraphAPI(access_token=ACCESS_TOKEN, version='2.9')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Find top pages.\n",
    "results = graph.search(type='page', q='Barack Obama', limit=5)\n",
    "top_result = results['data'][0]\n",
    "print('Top results: %s (ID = %s)' %(top_result['name'], top_result['id']))\n",
    "\n",
    "## Request page metadata.\n",
    "metadata = graph.get_object(top_result['id'], fields='id,name,about,link')\n",
    "\n",
    "## Request five most recent posts.\n",
    "post_results = graph.get_connections(top_result['id'], connection_name='posts', limit=5)\n",
    "\n",
    "## Iteratively request and store the first \n",
    "## five comments from each post.\n",
    "df = []\n",
    "for post in post_results['data']:\n",
    "    \n",
    "    ## Request comments.\n",
    "    comment_results = graph.get_connections(post['id'], connection_name='comments', limit=5,\n",
    "                                            fields='id,created_time,message,like_count')\n",
    "    \n",
    "    ## Iteratively convert comments to Series. Store.\n",
    "    comments = [Series(comment) for comment in comment_results['data']]\n",
    "    df += comments\n",
    "    \n",
    "## Convert into DataFrame.\n",
    "df = DataFrame(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API\n",
    "To use Twitter's API, you will first need to [register](https://apps.twitter.com/) for an application with Twitter. Once there, create and new app and fill out the requisite details (e.g. name, description, website, etc.). \n",
    "\n",
    "After registering, you will need your **Consumer Key** and **Consumuer Secret**, which will be used to authorize your API calls. Be sure to save in a protected manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Twitter\n",
    "The primary function with an OAuth2-authenticated Twitter instance is the search function, which returns relevant Tweets based on a query. There is substantial documentation by Twitter on this function, including:\n",
    "* Search function [docstring](https://dev.twitter.com/rest/reference/get/search/tweets)\n",
    "* Search function [syntax](https://dev.twitter.com/rest/public/search)\n",
    "* Search function [best uses](https://dev.twitter.com/rest/public/timelines)\n",
    "* Search function [rate limits](https://dev.twitter.com/rest/public/rate-limiting)\n",
    "\n",
    "### Function Call \n",
    "The search function accepts the following parameters:\n",
    "\n",
    "|Name|Required|Description|Example|\n",
    "|----|--------|-----------|-------|\n",
    "|q|Yes|A UTF-8, URL-encoded search query of 500 characters maximum, including operators. Queries may additionally be limited by complexity.| @szorowi1, python, #qmss|\n",
    "|geocode|No|Returns tweets by users located within a given radius of the given latitude/longitude. The location is preferentially taking from the Geotagging API, but will fall back to their Twitter profile. The parameter value is specified by ” latitude,longitude,radius ”, where radius units must be specified as either ” mi ” (miles) or ” km ” (kilometers).|40.81207 -73.954377 1mi|\n",
    "|lang|No|Restricts tweets to the given language, given by an [ISO 639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) code.|eu, en|\n",
    "|result_type|No|Specifies what type of search results you would prefer to receive.|recent, popular, mixed (default)|\n",
    "|count|No|The number of tweets to return per page, up to a maximum of 100.|15 (default)\n",
    "|until|No|Returns tweets created before the given date. Date should be formatted as YYYY-MM-DD. Keep in mind that the search index has a **7-day limit**: no tweets will be found for a date older than one week.|2015-07-19|\n",
    "|since_id|No|Returns results with an ID greater than (that is, more recent than) the specified ID. There are limits to the number of Tweets which can be accessed through the API. If the limit of Tweets has occured since the since_id, the since_id will be forced to the oldest ID available.|12345|\n",
    "|max_id|No|Returns results with an ID less than (that is, older than) or equal to the specified ID.|54321|\n",
    "\n",
    "### Search Range\n",
    "The **max_id** and **since_id** flags are important due to the [dynamic nature](https://dev.twitter.com/rest/public/timelines) of Twitter posts. In other words, the problem with Twitter is that new posts are constantly being added. Without properly specifing flags demarcating the start- and stop-points for searching, the risk of storing duplicate Tweets is high. To combat this when making multiple Search queries, the **ID - 1** of the most recently stored Tweet should be passed as the max_id flag for the next query. In this way, the next batch of queried Tweets will begin where the previous batch left off. The **since_id** flag can similarly be used to only add Tweets newer than the previously newest Tweet collected. For more details (and a more complete explanation, see [here](https://dev.twitter.com/rest/public/timelines)).\n",
    "\n",
    "### Syntax\n",
    "Twitter has conveniently [documented](https://dev.twitter.com/rest/public/search) the syntax of search queries. Some examples are given below:\n",
    "\n",
    "|Operator|Finds Tweets|\n",
    "|--------|------------|\n",
    "|watching now|containing both “watching” and “now”. This is the default operator.|\n",
    "|“happy hour”|containing the exact phrase “happy hour”.|\n",
    "|love OR hate|containing either “love” or “hate” (or both).\n",
    "|beer -root|containing “beer” but not “root”.\n",
    "|#haiku|containing the hashtag “haiku”.\n",
    "|from:interior|sent from Twitter account “interior”.\n",
    "|to:NASA|a Tweet authored in reply to Twitter account “NASA”.\n",
    "|@NASA|mentioning Twitter account “NASA”.\n",
    "|puppy filter:media|containing “puppy” and an image or video.\n",
    "|puppy -filter:retweets|containing “puppy”, filtering out retweets|\n",
    "|hilarious filter:links|containing “hilarious” and linking to URL.\n",
    "|puppy url:amazon|containing “puppy” and a URL with the word “amazon” anywhere within it.\n",
    "|superhero since:2015-12-21|containing “superhero” and sent since date “2015-12-21” (year-month-day).\n",
    "|puppy until:2015-12-21|containing “puppy” and sent before the date “2015-12-21”.\n",
    "|movie -scary :)|containing “movie”, but not “scary”, and with a positive attitude.\n",
    "|flight :(|containing “flight” and with a negative attitude.\n",
    "|traffic ?|containing “traffic” and asking a question.|\n",
    "\n",
    "### URL Encoded Queries\n",
    "Twitter requests that all search queries be [URL-encoded](https://en.wikipedia.org/wiki/Percent-encoding). Fortunately, this is not difficult with the **urllib** python library.\n",
    "\n",
    "\\> import urllib <br\\>> urllib.parse.quote_plus(\"query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twython\n",
    "From its [Github](https://github.com/ryanmcgrath/twython):\n",
    ">Twython is the premier Python library providing an easy (and up-to-date) way to access Twitter data. Actively maintained and featuring support for Python 2.6+ and Python 3. It's been battle tested by companies, educational institutions and individuals alike. \n",
    "\n",
    "To install Twython, simply open a Terminal and input: \n",
    "\n",
    "\\> pip install python\n",
    "\n",
    "The following examples will only provide a basic introduction to the steps necessary for mining Twitter data. For further reading, there are also a number of tutorials [here](http://nodotcom.org/python-twitter-tutorial.html), [here](http://socialmedia-class.org/twittertutorial.html), and [here](http://adilmoujahid.com/posts/2014/07/twitter-analytics/). \n",
    "\n",
    "### Initializing Twitter Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from twython import Twython\n",
    "\n",
    "## Load and store authentication keys.\n",
    "oath = np.load('twitter_oath.npz')\n",
    "APP_KEY = oath['consumer_key']\n",
    "APP_SECRET = oath['consumer_secret']\n",
    "\n",
    "## Initialize twitter object to receive ACCESS_TOKEN.\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "\n",
    "## Store and save ACCESS_TOKEN.\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "\n",
    "## Re-initilize Twitter object with ACCESS_TOKEN to use it.\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "## Search for Tweets using the hashtag python,\n",
    "## with positive connotation.\n",
    "query = '#python :)'\n",
    "\n",
    "## URL-encode query.\n",
    "query = urllib.parse.quote_plus(query)\n",
    "\n",
    "## Make function call.\n",
    "results = twitter.search(q=query, lang='en')\n",
    "type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query results are stored in a dictionary format. At the top level, there are two keys:\n",
    "* **statuses:** a list of dictionaries containg the results of the search.\n",
    "* **search_metadata:** a dictionary containing information about the query itself, including the max_id and since_id\n",
    "\n",
    "Each dictionary in \"statuses\" contains a large amount of information, including: \n",
    "* **Tweet ID:** id\n",
    "* **Tweet creation date:** created_at\n",
    "* **Tweet text:** text\n",
    "* **Retweet count:** retweet_count\n",
    "* **Favorite count:** favorite_count\n",
    "* **Coordinates:** coordinates (if available). \n",
    "\n",
    "\"statuses\" also contains several additional keys that store further dictionaries of information, including: \n",
    "* **User information:** user\n",
    "* **Tweet metadata:** metadata\n",
    "* **Content of retweeted status:** retweeted_status (if applicable)\n",
    "\n",
    "### Aggregating Search Results\n",
    "The list of dictionaries can very easily be amalgamated into a single Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:39 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385646454386692</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @craigbrownphd: #MachineLearning #Spark #Py...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 619148364, 'id_str': '619148364', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:12 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385531266277377</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Get started as a #Maker by signing up for our ...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 3925032559, 'id_str': '3925032559', 'na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:08 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385517139865600</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>This Hours Photo: #weather #minnesota #photo #...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 821038569791819776, 'id_str': '82103856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:03 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385496654880768</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @DataCamp: Infographic: switching from #web...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 4619684853, 'id_str': '4619684853', 'na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:03 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385495367229441</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Trend Crawler: added 4 trending topics to DB, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 855036706159984640, 'id_str': '85503670...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at  favorite_count  \\\n",
       "0         None        None  Sat Jun 10 03:45:39 +0000 2017               0   \n",
       "1         None        None  Sat Jun 10 03:45:12 +0000 2017               0   \n",
       "2         None        None  Sat Jun 10 03:45:08 +0000 2017               0   \n",
       "3         None        None  Sat Jun 10 03:45:03 +0000 2017               0   \n",
       "4         None        None  Sat Jun 10 03:45:03 +0000 2017               0   \n",
       "\n",
       "   favorited   geo                  id in_reply_to_screen_name  \\\n",
       "0      False  None  873385646454386692                    None   \n",
       "1      False  None  873385531266277377                    None   \n",
       "2      False  None  873385517139865600                    None   \n",
       "3      False  None  873385496654880768                    None   \n",
       "4      False  None  873385495367229441                    None   \n",
       "\n",
       "   in_reply_to_status_id in_reply_to_status_id_str  \\\n",
       "0                    NaN                      None   \n",
       "1                    NaN                      None   \n",
       "2                    NaN                      None   \n",
       "3                    NaN                      None   \n",
       "4                    NaN                      None   \n",
       "\n",
       "                         ...                          in_reply_to_user_id_str  \\\n",
       "0                        ...                                             None   \n",
       "1                        ...                                             None   \n",
       "2                        ...                                             None   \n",
       "3                        ...                                             None   \n",
       "4                        ...                                             None   \n",
       "\n",
       "  is_quote_status  lang place possibly_sensitive retweet_count  retweeted  \\\n",
       "0           False    en  None                NaN             1      False   \n",
       "1           False    en  None              False             0      False   \n",
       "2           False    en  None              False             0      False   \n",
       "3           False    en  None              False            11      False   \n",
       "4           False    en  None                NaN             0      False   \n",
       "\n",
       "                                                text truncated  \\\n",
       "0  RT @craigbrownphd: #MachineLearning #Spark #Py...     False   \n",
       "1  Get started as a #Maker by signing up for our ...      True   \n",
       "2  This Hours Photo: #weather #minnesota #photo #...     False   \n",
       "3  RT @DataCamp: Infographic: switching from #web...     False   \n",
       "4  Trend Crawler: added 4 trending topics to DB, ...     False   \n",
       "\n",
       "                                                user  \n",
       "0  {'id': 619148364, 'id_str': '619148364', 'name...  \n",
       "1  {'id': 3925032559, 'id_str': '3925032559', 'na...  \n",
       "2  {'id': 821038569791819776, 'id_str': '82103856...  \n",
       "3  {'id': 4619684853, 'id_str': '4619684853', 'na...  \n",
       "4  {'id': 855036706159984640, 'id_str': '85503670...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "\n",
    "## Convert each status dictionary into a Pandas Series.\n",
    "statuses = [Series(status) for status in results['statuses']]\n",
    "\n",
    "## Merge into single DataFrame\n",
    "df = DataFrame(statuses)\n",
    "\n",
    "## Drop unnecessary columns.\n",
    "df = df.drop(['entities', 'extended_entities', 'id_str', 'metadata', 'retweeted_status', 'source'], 1)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is especially easy now to lookup and access the content of Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     RT @craigbrownphd: #MachineLearning #Spark #Py...\n",
       "1     Get started as a #Maker by signing up for our ...\n",
       "2     This Hours Photo: #weather #minnesota #photo #...\n",
       "3     RT @DataCamp: Infographic: switching from #web...\n",
       "4     Trend Crawler: added 4 trending topics to DB, ...\n",
       "5     @JackieKazil @Podcast__init__ Here's @JackieKa...\n",
       "6     RT @AdamSmitht1: #python sklearn install error...\n",
       "7     #python sklearn install error can not be found...\n",
       "8     RT @gotynker: Let's beat the summer slide! Her...\n",
       "9     Check out @JackieKazil's inspiring interview o...\n",
       "10    #MachineLearning #Spark #Python #Java run on #...\n",
       "11    Python List Comprehension Vs. Map #python #lis...\n",
       "12    RT @Hakin9: Face Recognition With Python, in U...\n",
       "13    What is the standard Python docstring format? ...\n",
       "14    Why use pip over easy_install? #python #pip #s...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, several of the secondary dictionaries (e.g. entities, metadata, user) are stored as such in their respective columns. These can be deleted, or new functions can be written to extract relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:39 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385646454386692</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @craigbrownphd: #MachineLearning #Spark #Py...</td>\n",
       "      <td>False</td>\n",
       "      <td>619148364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:12 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385531266277377</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Get started as a #Maker by signing up for our ...</td>\n",
       "      <td>True</td>\n",
       "      <td>3925032559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:08 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385517139865600</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>This Hours Photo: #weather #minnesota #photo #...</td>\n",
       "      <td>False</td>\n",
       "      <td>821038569791819776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:03 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385496654880768</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @DataCamp: Infographic: switching from #web...</td>\n",
       "      <td>False</td>\n",
       "      <td>4619684853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:45:03 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385495367229441</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Trend Crawler: added 4 trending topics to DB, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>855036706159984640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at  favorite_count  \\\n",
       "0         None        None  Sat Jun 10 03:45:39 +0000 2017               0   \n",
       "1         None        None  Sat Jun 10 03:45:12 +0000 2017               0   \n",
       "2         None        None  Sat Jun 10 03:45:08 +0000 2017               0   \n",
       "3         None        None  Sat Jun 10 03:45:03 +0000 2017               0   \n",
       "4         None        None  Sat Jun 10 03:45:03 +0000 2017               0   \n",
       "\n",
       "   favorited   geo                  id in_reply_to_screen_name  \\\n",
       "0      False  None  873385646454386692                    None   \n",
       "1      False  None  873385531266277377                    None   \n",
       "2      False  None  873385517139865600                    None   \n",
       "3      False  None  873385496654880768                    None   \n",
       "4      False  None  873385495367229441                    None   \n",
       "\n",
       "   in_reply_to_status_id in_reply_to_status_id_str         ...          \\\n",
       "0                    NaN                      None         ...           \n",
       "1                    NaN                      None         ...           \n",
       "2                    NaN                      None         ...           \n",
       "3                    NaN                      None         ...           \n",
       "4                    NaN                      None         ...           \n",
       "\n",
       "   in_reply_to_user_id_str is_quote_status  lang place possibly_sensitive  \\\n",
       "0                     None           False    en  None                NaN   \n",
       "1                     None           False    en  None              False   \n",
       "2                     None           False    en  None              False   \n",
       "3                     None           False    en  None              False   \n",
       "4                     None           False    en  None                NaN   \n",
       "\n",
       "  retweet_count  retweeted                                               text  \\\n",
       "0             1      False  RT @craigbrownphd: #MachineLearning #Spark #Py...   \n",
       "1             0      False  Get started as a #Maker by signing up for our ...   \n",
       "2             0      False  This Hours Photo: #weather #minnesota #photo #...   \n",
       "3            11      False  RT @DataCamp: Infographic: switching from #web...   \n",
       "4             0      False  Trend Crawler: added 4 trending topics to DB, ...   \n",
       "\n",
       "  truncated                user  \n",
       "0     False           619148364  \n",
       "1      True          3925032559  \n",
       "2     False  821038569791819776  \n",
       "3     False          4619684853  \n",
       "4     False  855036706159984640  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define new user extract function.\n",
    "extract_user_info = np.vectorize( lambda d: d['id'] if isinstance(d,dict) else np.nan )\n",
    "\n",
    "## Apply.\n",
    "df.user = extract_user_info(df.user)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Twitter [rate limits](https://dev.twitter.com/rest/public/rate-limiting) API queries, meaning that we need to be aware of how often we are querying. According to [this table](https://dev.twitter.com/rest/public/rate-limits), it appears the OAuth 2 search function is limited to 450 requests per 15 minute window. In other words, we are limited to one request every 2 seconds. \n",
    "\n",
    "Below, we make use of the code previously written and the time library to make 10 search queries merge and store the results while obeying the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>metadata</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:44:29 +0000 2017</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 870437204333559811, 'id_str'...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385351292870657</td>\n",
       "      <td>873385351292870657</td>\n",
       "      <td>...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>163</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Fri Jun 02 00:29:37 +0000 2017...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @MadamMelanin: Kali Uchis | This singer fro...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1329443965, 'id_str': '1329443965', 'na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:43:30 +0000 2017</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873385106643320836</td>\n",
       "      <td>873385106643320836</td>\n",
       "      <td>...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://publicize.wp.com/\" rel=\"nofoll...</td>\n",
       "      <td>2 unaccounted for after aircraft goes missing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 795024244606283776, 'id_str': '79502424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sat Jun 10 03:42:22 +0000 2017</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>873384819970977793</td>\n",
       "      <td>873384819970977793</td>\n",
       "      <td>...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6860</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Thu Jun 08 16:16:27 +0000 2017...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @AlanDersh: Senators should ask Comey the n...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 3540150733, 'id_str': '3540150733', 'na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at  \\\n",
       "0         None        None  Sat Jun 10 03:44:29 +0000 2017   \n",
       "1         None        None  Sat Jun 10 03:43:30 +0000 2017   \n",
       "2         None        None  Sat Jun 10 03:42:22 +0000 2017   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "1  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                   extended_entities  favorite_count  \\\n",
       "0  {'media': [{'id': 870437204333559811, 'id_str'...               0   \n",
       "1                                                NaN               0   \n",
       "2                                                NaN               0   \n",
       "\n",
       "   favorited   geo                  id              id_str  \\\n",
       "0      False  None  873385351292870657  873385351292870657   \n",
       "1      False  None  873385106643320836  873385106643320836   \n",
       "2      False  None  873384819970977793  873384819970977793   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "\n",
       "                                            metadata  place  \\\n",
       "0  {'iso_language_code': 'en', 'result_type': 're...   None   \n",
       "1  {'iso_language_code': 'en', 'result_type': 're...   None   \n",
       "2  {'iso_language_code': 'en', 'result_type': 're...   None   \n",
       "\n",
       "  possibly_sensitive  retweet_count retweeted  \\\n",
       "0              False            163     False   \n",
       "1              False              0     False   \n",
       "2                NaN           6860     False   \n",
       "\n",
       "                                    retweeted_status  \\\n",
       "0  {'created_at': 'Fri Jun 02 00:29:37 +0000 2017...   \n",
       "1                                                NaN   \n",
       "2  {'created_at': 'Thu Jun 08 16:16:27 +0000 2017...   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://publicize.wp.com/\" rel=\"nofoll...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text truncated  \\\n",
       "0  RT @MadamMelanin: Kali Uchis | This singer fro...     False   \n",
       "1  2 unaccounted for after aircraft goes missing ...     False   \n",
       "2  RT @AlanDersh: Senators should ask Comey the n...     False   \n",
       "\n",
       "                                                user  \n",
       "0  {'id': 1329443965, 'id_str': '1329443965', 'na...  \n",
       "1  {'id': 795024244606283776, 'id_str': '79502424...  \n",
       "2  {'id': 3540150733, 'id_str': '3540150733', 'na...  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time, urllib\n",
    "import numpy as np\n",
    "from twython import Twython\n",
    "from pandas import Series, DataFrame, concat\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Authentication.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load and store authentication keys.\n",
    "oath = np.load('twitter_oath.npz')\n",
    "APP_KEY = oath['consumer_key']\n",
    "APP_SECRET = oath['consumer_secret']\n",
    "\n",
    "## Initialize twitter object to receive ACCESS_TOKEN.\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "\n",
    "## Store and save ACCESS_TOKEN.\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "\n",
    "## Re-initilize Twitter object with ACCESS_TOKEN to use it.\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Search for Tweens from Columbia University\n",
    "## filtering out any retweets.\n",
    "query = 'from:columbia -filter:retweets'\n",
    "\n",
    "## URL-encode query.\n",
    "query = urllib.parse.quote_plus(query)\n",
    "\n",
    "max_id = False\n",
    "for _ in range(10):\n",
    "    \n",
    "    ## Make function call.\n",
    "    if not max_id: results = twitter.search(q=query, lang='en')\n",
    "    else: results = twitter.search(q=query, lang='en', max_id = max_id-1)\n",
    "        \n",
    "    ## Convert each status dictionary into a Pandas Series.\n",
    "    statuses = [Series(status) for status in results['statuses']]\n",
    "    \n",
    "    ## Merge.\n",
    "    if not max_id: df = DataFrame(statuses)\n",
    "    else: df = concat([df,DataFrame(statuses)])\n",
    "        \n",
    "    ## Extract max_id\n",
    "    max_id = results['search_metadata']['max_id']\n",
    "        \n",
    "    ## Sleep timer.\n",
    "    time.sleep(2)\n",
    "\n",
    "## Display results.\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit API\n",
    "To use Reddit's API, you will first need to [register](https://github.com/reddit/reddit/wiki/OAuth2) for an application with Twitter. Once there, create a new app and fill out the requisite details (e.g. name, description, etc.). \n",
    "\n",
    "After registering, you will need your **Client ID** and **Consumuer Secret**, which will be used to authorize your API calls. Be sure to save in a protected manner. In addition, you will need to define a **user agent** code that follows the naming conventions outlined [here](https://github.com/reddit/reddit/wiki/API#rules).\n",
    "\n",
    "The complete [Reddit API](https://www.reddit.com/dev/api/) is a helpful resource, but is predominantly full of information of user-authenticated services. We will largely focus on OAuth 2 operations, i.e. searching through publicly accessible Reddit content.\n",
    "\n",
    "## PRAW\n",
    "The Python Reddit API Wrapper, or [PRAW](https://github.com/praw-dev/praw), is:\n",
    ">... a python package that allows for simple access to Reddit's API. PRAW aims to be easy to use and internally follows all of Reddit's API rules. With PRAW there's no need to introduce sleep calls in your code. Give your client an appropriate user agent and you're set.\n",
    "\n",
    "To install PRAW, simply open a terminal and input:\n",
    "\n",
    "\\> pip install praw\n",
    "\n",
    "For further reading, there are also a number of tutorials [here](https://praw.readthedocs.io/en/latest/tutorials/comments.html), [here](http://pythonforengineers.com/build-a-reddit-bot-part-1/), and [here](http://minimaxir.com/2015/10/reddit-bigquery/).\n",
    "\n",
    "\n",
    "### Initializing  Reddit Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<praw.reddit.Reddit at 0x7fa6c57955c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from praw import Reddit\n",
    "\n",
    "## Load and store authentication keys.\n",
    "oath = np.load('reddit_oath.npz')\n",
    "CLIENT_ID = oath['client_id'].astype(str).tolist()\n",
    "CLIENT_SECRET = oath['client_secret'].astype(str).tolist()\n",
    "USER_AGENT = oath['user_agent'].astype(str).tolist()\n",
    "\n",
    "## Initialize Reddit object to receive ACCESS_TOKEN.\n",
    "reddit = Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent=USER_AGENT)\n",
    "reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Querying \n",
    "To understand querying for Reddit, one first has to know a little about the organization of Reddit itself. For the uninitiated, Reddit is a forum website comprised of multiple specialty forums, known as **subreddits**. An individual subreddit is, in itself made up of **submissions**, or user posts to the subreddit. Submissions are made of the **body**, or content, of the post and **comments** left in response to the post. **Users** generate submissions and comments, and are also members of different subreddits.\n",
    "\n",
    "Each of these individual instances (e.g. subreddits, submissions, comments, users) are queryable. The pseudo-hierarchical structure of Reddit, however, means that querying a higher-order class will usually return lower level classes (e.g. querying a subreddit will return submissions, querying a submission will return comments). Reflecting the structure of Reddit, we will cover each type of query in a top-down fashion.\n",
    "\n",
    "### Querying Subreddits\n",
    "To search over the space of subreddits (i.e. over all forums), there are two main functions: **subreddits** and **random_subreddit**. The former provides different tools for finding subreddits, including providing a list of default and popular subreddits; the latter randomly selects and returns an instance of one subreddit. If performing search, Reddit's search syntax can be found [here](https://www.reddit.com/wiki/search). Below are examples of both function types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Subreddit(display_name='avocadosgonewild'),\n",
       " Subreddit(display_name='avocado'),\n",
       " Subreddit(display_name='avocados'),\n",
       " Subreddit(display_name='enlightenedavocadomen'),\n",
       " Subreddit(display_name='unexpectedavocado')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Search for subreddits involving avocados.\n",
    "avocado_subreddits = reddit.subreddits.search('avocado')\n",
    "avocado_subreddits = list(avocado_subreddits)\n",
    "avocado_subreddits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subreddit(display_name='occupywallstreet')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Return random subreddit.\n",
    "random_subreddit = reddit.random_subreddit()\n",
    "random_subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Submissions\n",
    "After identifying a subreddit of interest, submissions are queried. Submissions can be queried directly from a subreddit instance using the **subreddit.submissions** attribute.\n",
    "\n",
    "Similar to querying for subreddits, querying submissions of a subreddit will return a generator instance. This can be traversed to return submissions to that subreddit. The submissions function has three attributes:\n",
    "* **start:** A UNIX timestamp indicating the earliest creation time of submission yielded during the call.\n",
    "* **end:** A UNIX timestamp indicating the latest creation time of a submission yielded during the call\n",
    "* **extra_query:**  cloudsearch query that will be used to further filter results.\n",
    "\n",
    "We demonstrate with the r/learnpython subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Submission(id='6es1xr'),\n",
       " Submission(id='6erq6p'),\n",
       " Submission(id='6ermnb'),\n",
       " Submission(id='6er5ub'),\n",
       " Submission(id='6er3b9')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime, time\n",
    "\n",
    "## Initialize subreddit instance.\n",
    "subreddit = reddit.subreddit('learnpython')\n",
    "\n",
    "## Define start and end dates \n",
    "start = '2017-06-01'\n",
    "start = time.mktime(datetime.datetime.strptime(start, '%Y-%m-%d').timetuple()) # Convert to UNIX time.\n",
    "end = '2017-06-02'\n",
    "end = time.mktime(datetime.datetime.strptime(end, '%Y-%m-%d').timetuple()) # Convert to UNIX time.\n",
    "\n",
    "## Query submissions.\n",
    "submissions = subreddit.submissions(start=start, end=end)\n",
    "submissions = list(submissions)\n",
    "submissions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Bodies & Comments\n",
    "Once a submission has been identified, its content can be queried. Note from above that submissions are identified by their respective **IDs**. \n",
    "\n",
    "At the level of submission there are many stored pieces of data, including: author, title, subreddit, and upvotes/downvotes/score. There is also the body of the submission, stored as its **selftext**. Finally, the comments on the submission can also be investigated. These have similar attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: trexmixx\n",
      "Title: Running a python file continuously (best practice)\n",
      "Score: 46\n",
      "Body: Somewhat new to practical python.  I'm working on making a twitter bot- whats the best practice for keeping a script running continuously?  I've looked into hosting it on a free web server online and getting a raspberry pi and keeping it on - are either of these actually legitimate?  I'm out of my depth.\n"
     ]
    }
   ],
   "source": [
    "## Take first submission.\n",
    "submission = submissions[1]\n",
    "\n",
    "## Print metadata.\n",
    "print('Author: %s' %submission.author)\n",
    "print('Title: %s' %submission.title)\n",
    "print('Score: %s' %submission.score)\n",
    "print('Body: %s' %submission.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract comments.\n",
    "comments = submission.comments\n",
    "comments = list(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: elbiot\n",
      "Score: 21\n",
      "Body: Either of those are fine options. Write a systemd service, since that's how continually running services are usually done on Linux (now that systemd is in almost all of the major distros).\n"
     ]
    }
   ],
   "source": [
    "## Take first comment.\n",
    "comment = comments[0]\n",
    "\n",
    "## Print metadata.\n",
    "print('Author: %s' %comment.author)\n",
    "print('Score: %s' %comment.score)\n",
    "print('Body: %s' %comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Due to the hierarchical nature of Reddit, data organization/storage is a non-trivial design problem and shoudl reflect the ultimate analytic goals. For example, if one is interested only in text mining the submissions of subreddit without caring about the structure of submission-comment and comment-comment relationships, then a flat structure may be appropriate. If more complex tree-like relations are desired, then nested data structures (e.g. nested file directories, JSON, XML, etc.) may be necesary. \n",
    "\n",
    "In any event, an example script is given below that mines submissions & comments from r/Anxiety, as part of a larger project of predicting symptoms clusters from text. It should also be noted that PRAW handles Reddit's [rate limit](https://github.com/reddit/reddit/wiki/API#rules) of 60 requests per minute (1 request/sec; [source](https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html)).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iwabo1234</td>\n",
       "      <td>1.496376e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>I'm getting my wisdom teeth removed tomorrow (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>winning34</td>\n",
       "      <td>1.496382e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>Dental anxiety is a very real thing, but thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kajjaznam1</td>\n",
       "      <td>1.496408e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>I took one out 2 days ago, it didn't hurt beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barbar21</td>\n",
       "      <td>1.496375e+09</td>\n",
       "      <td>7</td>\n",
       "      <td>So long story short i think i have anxiety iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solidpancake</td>\n",
       "      <td>1.496383e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>Once I heard it described as that feeling you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           User          Date  Score  \\\n",
       "0     iwabo1234  1.496376e+09      2   \n",
       "1     winning34  1.496382e+09      2   \n",
       "2    kajjaznam1  1.496408e+09      2   \n",
       "3      Barbar21  1.496375e+09      7   \n",
       "4  solidpancake  1.496383e+09      5   \n",
       "\n",
       "                                                Text  \n",
       "0  I'm getting my wisdom teeth removed tomorrow (...  \n",
       "1  Dental anxiety is a very real thing, but thank...  \n",
       "2  I took one out 2 days ago, it didn't hurt beca...  \n",
       "3  So long story short i think i have anxiety iss...  \n",
       "4  Once I heard it described as that feeling you ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime, time\n",
    "from praw import Reddit\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Authentication.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load and store authentication keys.\n",
    "oath = np.load('reddit_oath.npz')\n",
    "CLIENT_ID = oath['client_id'].astype(str).tolist()\n",
    "CLIENT_SECRET = oath['client_secret'].astype(str).tolist()\n",
    "USER_AGENT = oath['user_agent'].astype(str).tolist()\n",
    "\n",
    "## Initialize Reddit object to receive ACCESS_TOKEN.\n",
    "reddit = Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent=USER_AGENT)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize posts (bodies + comments).\n",
    "posts = []\n",
    "\n",
    "## Define start and end dates \n",
    "start = '2017-06-01'\n",
    "start = time.mktime(datetime.datetime.strptime(start, '%Y-%m-%d').timetuple()) # Convert to UNIX time.\n",
    "end = '2017-06-02'\n",
    "end = time.mktime(datetime.datetime.strptime(end, '%Y-%m-%d').timetuple()) # Convert to UNIX time.\n",
    "\n",
    "## Load subreddit.\n",
    "subreddit = reddit.subreddit('Anxiety')\n",
    "\n",
    "for submission in subreddit.submissions(start=start, end=end):\n",
    "    \n",
    "    ## Parse submission.\n",
    "    series = Series([submission.author, submission.created_utc, submission.score, submission.selftext],\n",
    "                    index=['User','Date','Score','Text'])\n",
    "    posts.append(series)\n",
    "    \n",
    "    ## Iterate over comments.\n",
    "    for comment in submission.comments:\n",
    "        \n",
    "        ## Parse submission.\n",
    "        series = Series([comment.author, comment.created_utc, comment.score, comment.body],\n",
    "                        index=['User','Date','Score','Text'])\n",
    "        posts.append(series)\n",
    "        \n",
    "\n",
    "## Merge.\n",
    "posts = DataFrame(posts)\n",
    "posts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Python API Wrappers\n",
    "The [Real Python](https://realpython.com/) group has put together an extensive [list of Python API wrappers](https://github.com/realpython/list-of-python-api-wrappers), which includes Amazon Shopping, Amazon Web Services, Craigslist, Dropbox, Ebay, Evernote, FedEx, Flickr, Forecast, Geopy, Github, Google Maps, Google Music, Indeed, Instagram, Linkedin, Medium, NASA, Netflix, RottenTomatoes, Slack, Spotify, StackExchange, Uber, World Bank, Yahoo, and YouTube. See the list for full details including links.\n",
    "\n",
    "Outside of the list, several miscellaneous Python API wrappers stumbled across while writing this tutorial include the [Federal Election Committee](https://github.com/jeremyjbowers/pyopenfec), [Elsevier](https://github.com/ElsevierDev/elsapy), [Google Scholar](https://github.com/ckreibich/scholar.py), [IMDB](http://imdbpy.sourceforge.net/), and [Last.fm](https://github.com/pylast/pylast)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "When API access to online content is not available, webscraping is a viable alternative in Python. There are many available packages and tools for webscraping with Python, and at the time of writing there are two  dominant packages: [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#) and [Scrapy](https://scrapy.org/). \n",
    "\n",
    "**Scrapy** is a newer package that specifically handles the webcrawling aspect of webscraping; it was written specifically to easily and efficiently crawl many webpages and store content for later parsing. **BeautifulSoup** is a library designed to parse HTML, CSS, and other web-based programming languages. A more complete treatment of webcrawling and web-based programming languages is beyond the scope of the current tutorial, but a working example of BeautifulSoup is given below. For more detailed introductions to Scrapy and BeautifulSoup, please see their [respective](https://docs.scrapy.org/en/latest/index.html) [documentations](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#) as well as these tutorials on [webcrawling](https://docs.scrapy.org/en/latest/intro/tutorial.html#) and [webscraping](https://www.dataquest.io/blog/web-scraping-tutorial-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "In this example, we will use BeautifulSoup to webscrape the table of [complete music ratings](http://www.albumoftheyear.org/ratings/overall/2017/) from the website, [*Album of the Year*](http://www.albumoftheyear.org/).\n",
    "\n",
    "### Requesting and Parsing HTML\n",
    " We will use the **requests** package to request and store the website information and use **BeautifulSoup** to parse the resulting HTML. (To note, Scrapy would replace the requests package in a fuller treatment of the subject.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:fb=\"http://ogp.me/ns/fb#\">\n",
      " <head>\n",
      "  <title>\n",
      "   Complete Music Ratings of 2017\n",
      "  </title>\n",
      "  <link href=\"/main.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <meta content=\"http://www.albumoftheyear.org/images/fbLogo.jpg\" property=\"og:image\"/>\n",
      "  <link href=\"https://fonts.googleapis.com/css?family=Open+Sans:400,700\" rel=\"stylesheet\"/>\n",
      "  <link href=\"http://cdn.albumoftheyear.org/images/favicon.png\" rel=\"icon\" type=\"image/png\"/>\n",
      "  <script data-cfasync=\"false\" src=\"https://s3.amazonaws.com/pubfig/albumoftheyear/pubfig1.min.js\">\n",
      "  </script>\n",
      " </head>\n",
      " <body id=\"graybg\">\n",
      "  <noscript>\n",
      "   <iframe height=\"0\" src=\"//www.googletagmanager.com/ns.html?id=GTM-TPD5DD\" style=\"display:none;visibility:hidden\" width=\"0\">\n",
      "   </iframe>\n",
      "  </noscript>\n",
      "  <script>\n",
      "   (function(w,d\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Define URL for scraping.\n",
    "url = 'http://www.albumoftheyear.org/ratings/overall/2017/'\n",
    "\n",
    "## Make request.\n",
    "request = requests.get(url)\n",
    "\n",
    "## Define a new BeautifulSoup object from the HTML \n",
    "## of the requested website, and specifying lxml\n",
    "## as the background library for parsing.\n",
    "soup = BeautifulSoup(request.text, 'lxml')\n",
    "\n",
    "## Call the prettify function to print the HTML in a\n",
    "## human readable format.\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printed above is the first 1000 characters of the HTML making up the [2017 table of complete music rankings](http://www.albumoftheyear.org/ratings/overall/2017/) from AOTY. HTML, or the HyperText Markup Language, is a markup language that tells a browser how to layout content. HTML specifies all of the content of a webpage, as well as its formatting. \n",
    "\n",
    "The HTML language is comprised of tags, which are denoted by \"</>\". There are many different types of tags in HTML and they denote different usages (e.g. `\"<p>\"` is the paragraph tag, `<div>` is the division tag,  `<a>` is the hyperlink tag). Tags may be passed parameters, which are properties of the tag and modify their function. Finally, HTML has a hierarchical format; in other words, tags are nested within other tags. As can be observed above, the `<title>` tag is nested under the `<head>` tag, and the `<head>` tag is nested under the ultimal `<html>` tag.\n",
    "\n",
    "The utility of BeautifulSoup is in parsing these nested hierarchical structures. A BeautifulSoup instance provides many functions for searching along and retrieving data from trees of tags. For example, tag types can be called directly from the soup instance. These will return the first tag matching the specified pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/recommendations/\">Recommendations</a>\n",
      "<br clear=\"all\"/>\n"
     ]
    }
   ],
   "source": [
    "## Get the first instance of two tags.\n",
    "print(soup.a)\n",
    "print(soup.br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the tags of a particular type can also be queried and returned in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/recommendations/\">Recommendations</a>,\n",
       " <a href=\"/add-album.php\" rel=\"nofollow\">Add Album</a>,\n",
       " <a href=\"/account/\" rel=\"nofollow\">Sign Up / Login</a>,\n",
       " <a href=\"http://www.facebook.com/albumoftheyear\" rel=\"nofollow\" target=\"_blank\"><div class=\"facebookHead\"></div></a>,\n",
       " <a href=\"http://www.twitter.com/aoty\" rel=\"nofollow\" target=\"_blank\"><div class=\"twitterHead\"></div></a>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find all a-tags, i.e. hyperlinks.\n",
    "tags = soup.find_all('a')\n",
    "tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indivdual tags have attributes that allow for further information retrieval, including the lookup of tags nested underneath it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag attrs: {'href': 'http://forums.albumoftheyear.org/c/site-feedback'}\n",
      "Tag href: http://forums.albumoftheyear.org/c/site-feedback\n",
      "Tag conents: ['Feedback']\n",
      "Tag text: Feedback\n"
     ]
    }
   ],
   "source": [
    "## Extract last tag from list.\n",
    "tag = tags[-1]\n",
    "\n",
    "## Print data from tag.\n",
    "print('Tag attrs: %s' %tag.attrs)\n",
    "print('Tag href: %s' %tag['href'])\n",
    "print('Tag conents: %s' %tag.contents)\n",
    "print('Tag text: %s' %tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some study of the HTML structure reveals that the start of each row of the [ratings table](http://www.albumoftheyear.org/ratings/overall/2017/) is denoted by the `<tr>` tag. We will use this information and the *descendants* attribute (which iteratively traverses through the children, grandchildren, great-grandchildren, etc. of the tag) to each the content from each row the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AoTY', 'Artist / Album', 'A.V.', 'AMG', 'CoS', 'DIY', 'DiS', 'OMH', 'NME', 'NoRip', 'Paste', 'P4K', 'PM', 'PMA', 'RS', 'Spin', '405', 'TLOBF', 'Skinny', 'TMT', 'Radar']\n",
      "['1', '92', 'Kendrick Lamar', 'DAMN.', '100', '90', '91', '100', '90', 'n/a', '80', 'n/a', '91', '92', '90', '100', '90', 'n/a', '95', 'n/a', 'n/a', '100', '80']\n",
      "['2', '90', 'Mount Eerie', 'A Crow Looked at Me', '91', '80', '91', 'n/a', 'n/a', 'n/a', 'n/a', '90', '92', '90', '100', '83', 'n/a', 'n/a', '90', 'n/a', 'n/a', '100', '80']\n",
      "['3', '86', 'Arca', 'Arca', '91', '80', '83', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', '85', '90', '83', 'n/a', 'n/a', '75', '85', 'n/a', '80', 'n/a']\n",
      "['4', '86', 'Valerie June', 'The Order of Time', 'n/a', '80', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', '90', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a']\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for link in soup.find_all('tr'):\n",
    "    rows.append( [descendant for descendant in link.descendants if isinstance(descendant,str)] )\n",
    "\n",
    "for row in rows[:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Search Results\n",
    "We can take the scraped table and store it in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>AoTY</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>A.V.</th>\n",
       "      <th>AMG</th>\n",
       "      <th>CoS</th>\n",
       "      <th>DIY</th>\n",
       "      <th>DiS</th>\n",
       "      <th>OMH</th>\n",
       "      <th>...</th>\n",
       "      <th>P4K</th>\n",
       "      <th>PM</th>\n",
       "      <th>PMA</th>\n",
       "      <th>RS</th>\n",
       "      <th>Spin</th>\n",
       "      <th>405</th>\n",
       "      <th>TLOBF</th>\n",
       "      <th>Skinny</th>\n",
       "      <th>TMT</th>\n",
       "      <th>Radar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>Kendrick Lamar</td>\n",
       "      <td>DAMN.</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>95</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>Mount Eerie</td>\n",
       "      <td>A Crow Looked at Me</td>\n",
       "      <td>91</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>Arca</td>\n",
       "      <td>Arca</td>\n",
       "      <td>91</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>83</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>75</td>\n",
       "      <td>85</td>\n",
       "      <td>n/a</td>\n",
       "      <td>80</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>Valerie June</td>\n",
       "      <td>The Order of Time</td>\n",
       "      <td>n/a</td>\n",
       "      <td>80</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>...</td>\n",
       "      <td>n/a</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>Jlin</td>\n",
       "      <td>Black Origami</td>\n",
       "      <td>n/a</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>80</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>80</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>80</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank AoTY          Artist                Album A.V. AMG  CoS  DIY  DiS  OMH  \\\n",
       "0    1   92  Kendrick Lamar                DAMN.  100  90   91  100   90  n/a   \n",
       "1    2   90     Mount Eerie  A Crow Looked at Me   91  80   91  n/a  n/a  n/a   \n",
       "2    3   86            Arca                 Arca   91  80   83  n/a  n/a  n/a   \n",
       "3    4   86    Valerie June    The Order of Time  n/a  80  n/a  n/a  n/a  n/a   \n",
       "4    5   86            Jlin        Black Origami  n/a  90   91  n/a  n/a  n/a   \n",
       "\n",
       "   ...   P4K   PM  PMA   RS Spin  405 TLOBF Skinny  TMT Radar  \n",
       "0  ...    92   90  100   90  n/a   95   n/a    n/a  100    80  \n",
       "1  ...    90  100   83  n/a  n/a   90   n/a    n/a  100    80  \n",
       "2  ...    85   90   83  n/a  n/a   75    85    n/a   80   n/a  \n",
       "3  ...   n/a   90  n/a  n/a  n/a  n/a   n/a    n/a  n/a   n/a  \n",
       "4  ...    88   80  n/a  n/a  n/a   80   n/a    n/a   80   n/a  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "## Define colnames (with a bit of hacking).\n",
    "colnames = ['Rank'] + [col for sublist in rows[0] for col in sublist.replace(' ','').split('/')] \n",
    "\n",
    "## Store contents as DataFrame, skipping over rows\n",
    "## identical to the first row.\n",
    "df = DataFrame([row for row in rows if not row == rows[0]], columns=colnames)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "All of this code can be assembled into one script that will act as a simple webcrawler and webscraper for the past and present AOTY tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Rank</th>\n",
       "      <th>AoTY</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>A.V.</th>\n",
       "      <th>AMG</th>\n",
       "      <th>CoS</th>\n",
       "      <th>DIY</th>\n",
       "      <th>DiS</th>\n",
       "      <th>...</th>\n",
       "      <th>P4K</th>\n",
       "      <th>PM</th>\n",
       "      <th>PMA</th>\n",
       "      <th>RS</th>\n",
       "      <th>Spin</th>\n",
       "      <th>405</th>\n",
       "      <th>TLOBF</th>\n",
       "      <th>Skinny</th>\n",
       "      <th>TMT</th>\n",
       "      <th>Radar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>Kendrick Lamar</td>\n",
       "      <td>To Pimp a Butterfly</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>D'Angelo</td>\n",
       "      <td>Black Messiah</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>n/a</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>100</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>Deafheaven</td>\n",
       "      <td>Sunbather</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>100</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>Sufjan Stevens</td>\n",
       "      <td>Carrie &amp; Lowell</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>Mbongwana Star</td>\n",
       "      <td>From Kinshasa</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>90</td>\n",
       "      <td>n/a</td>\n",
       "      <td>85</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Rank AoTY          Artist                Album A.V.  AMG  CoS  DIY  \\\n",
       "0  2015    1   94  Kendrick Lamar  To Pimp a Butterfly   91  100  100   80   \n",
       "0  2014    1   93        D'Angelo        Black Messiah  100   90   91  n/a   \n",
       "0  2013    1   91      Deafheaven            Sunbather  100   90   90  n/a   \n",
       "1  2015    2   90  Sufjan Stevens      Carrie & Lowell  100   80  100   80   \n",
       "2  2015    3   90  Mbongwana Star        From Kinshasa  n/a  n/a  n/a  n/a   \n",
       "\n",
       "   DiS  ...   P4K   PM  PMA   RS Spin  405 TLOBF Skinny  TMT Radar  \n",
       "0  100  ...    93   90  100   90  100  100    90    n/a  100    90  \n",
       "0   90  ...    94   90  100   90   90   90    90    n/a  100   n/a  \n",
       "0  n/a  ...    89   90  n/a   60   80   90   n/a    100  n/a   n/a  \n",
       "1   90  ...    93   60  100   80   80   95    85     80  100    85  \n",
       "2  100  ...   n/a  n/a  n/a  n/a   90  n/a    85    n/a  n/a   n/a  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame, concat\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define URL base (notice the string substituion).\n",
    "url = 'http://www.albumoftheyear.org/ratings/overall/%s/'\n",
    "\n",
    "## Define years.\n",
    "years = range(2013,2017)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "tables = []\n",
    "for year in years:\n",
    "    \n",
    "    ## Make request.\n",
    "    request = requests.get(url %year)\n",
    "\n",
    "    ## Define a new BeautifulSoup object.\n",
    "    soup = BeautifulSoup(request.text, 'lxml')\n",
    "\n",
    "    ## Extract all rows from the table.\n",
    "    rows = []\n",
    "    for link in soup.find_all('tr'):\n",
    "        rows.append( [descendant for descendant in link.descendants if isinstance(descendant,str)] )\n",
    "        \n",
    "    ## Define colnames (with a bit of hacking).\n",
    "    colnames = ['Rank'] + [col for sublist in rows[0] for col in sublist.replace(' ','').split('/')] \n",
    "\n",
    "    ## Store contents as DataFrame, skipping over rows\n",
    "    ## identical to the first row.\n",
    "    df = DataFrame([row for row in rows if not row == rows[0]], columns=colnames)\n",
    "    \n",
    "    ## Define year variable.\n",
    "    df.insert(0, 'Year', year)\n",
    "    \n",
    "    ## Store.\n",
    "    tables.append(df)\n",
    "    \n",
    "## Concatenate and sort by AOTY score.\n",
    "tables = concat(tables)\n",
    "tables = tables.sort_values(['AoTY'], ascending=False)\n",
    "tables.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "814px",
    "left": "0px",
    "right": "1057px",
    "top": "106px",
    "width": "164px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
