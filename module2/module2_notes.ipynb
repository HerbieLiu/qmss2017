{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "From its [documentation](http://pandas.pydata.org/pandas-docs/stable/):\n",
    "> pandas, or the Python Data Analysis Toolkit, is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. A\n",
    "\n",
    "> pandas is well suited for many different kinds of data:\n",
    "* Tabular data with heterogeneously-typed columns (e.g. Excel, SQL)\n",
    "* Ordered and unordered time series data.\n",
    "* Arbitrary matrix data with row and column labels\n",
    "* Any other form of observational / statistical data sets\n",
    "\n",
    "> The two primary data structures of pandas, **Series** (1-dimensional) and **DataFrame** (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. For R users, DataFrame provides everything that R’s data.frame provides and much more. \n",
    "\n",
    ">pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.\n",
    "\n",
    "Pandas is a powerful library that dramatically expands upon the NumPy library to intuitively and efficiently manipulate and analyze data. Pandas is a core package in the toolkit of any python-savvy researcher and will be used in this and all subsequent modules of the tutorial. \n",
    "\n",
    "## Series\n",
    "### Generating Series\n",
    "The **Series** object is one of two atomic units of Pandas. In fact, it's the object from DataFrames are generated. In a word, **Series** are extensions of NumPy arrays with dictionary-like properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import Series\n",
    "\n",
    "## Define a basic array.\n",
    "arr = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "## Convert to Series.\n",
    "series = Series(arr)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, Series store arrays with indices. In this case, the indices were autogenerated as a list of integers starting from 0. Let us make the indices more explict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "series = Series(arr, index=indices)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series can also be generated from pythonic dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(a = 0.0, b = 0.2, c = 0.4, d = 0.6, e = 0.8, f = 1.0)\n",
    "series = Series(d)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Series\n",
    "With indices, Series are a hybrid NumPy array and pythonic dictionary. The indexing/slicing behavior of Series supports this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lookup like Python dictionary\n",
    "series['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Index like NumPy array.\n",
    "series[[1,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Slice like NumPy array.\n",
    "series[:3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like NumPy arrays, Pandas Series can be directly manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series > 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a conditional to transform the values of a Series into Boolean types is helpful for indexing into Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series[series > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like dictionaries, the respective components of a Series object (indices, array) can be separately accessed. This can be used to update elements of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Series back into NumPy array.\n",
    "series.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access values of Series.\n",
    "series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access index of Series.\n",
    "series.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update Series index.\n",
    "series.index = ['i1','i2','i3','i4','i5','i6']\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical operations can be applied to two (or more) Series. Importantly, Series align themselves over their indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define three new series.\n",
    "s1 = series[:3]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = series[:3]\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = series[2:-1]\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute overlap of indices (no problem!).\n",
    "s1 * s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial overlap of indices (missing data!).\n",
    "s1 * s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New elements can be added to a Series \n",
    "series['i7'] = 5.\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Attributes\n",
    "Similar to NumPy arrays, Series objects have a number of useful attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize new series.\n",
    "arr = [2.5, 1.0, np.nan, 3.1, 0.5, 1.0]\n",
    "indices = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "series = Series(arr, index=indices)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loc** attribute allows for indexing similar to using brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.loc['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **iloc** attribute allows for indexing by element position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **head** attribute displays the first elements of a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tail** attribute displays the last elements of a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **isnull** attribute can be used to check for NaN (missing) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, **notnull** checks for non-NaN (non-missing) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute **fillna** fills all NaN (missing) data with a specified value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute **dropna** removes NaN (missing) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **drop** attribute removes data specified by their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.drop(['b','d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **unique** attribute returns the unique set of members from the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **value_counts** attribute returns the unique set of members from the Series and their respective frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series also have all of the standard summary attributes (e.g. sum, cumsum, max, min, mean, std, median, mode) and also has the **describe** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **append** attributes allows for multiple Series to be joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.append(series, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "### Generating DataFrames\n",
    "In all likelihood, the average analyst will spend very little time working with Series. Instead, the power of Pandas is in the DataFrame, which reproduces the object of the same name from R. Series are important because DataFrames are, in essence, just a series of Series; in other words, DataFrames are a collection of series, each comprising a separate column, and aligned on the index. \n",
    "\n",
    "Similar to Series, DataFrames can be initialized from lists, arrays, or dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "## Make data (5 rows, 2 cols).\n",
    "arr = [[8, 5], [0, 3], [9, 7], [6, 5], [2, 8]]\n",
    "\n",
    "## Convert to DataFrame.\n",
    "df = DataFrame(arr, columns=['foxtrot','yankee'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can also be initialized from dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict( foxtrot = [8, 0, 9, 6, 2],\n",
    "          yankee = [5, 3, 7, 5, 8] )\n",
    "\n",
    "## Conver to DataFrame\n",
    "df = DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices can also easily be added to DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make data (5 rows, 2 cols).\n",
    "arr = [[8, 5], [0, 3], [9, 7], [6, 5], [2, 8]]\n",
    "\n",
    "## Convert to DataFrame.\n",
    "df = DataFrame(arr, columns=['foxtrot','yankee'], index=['a','b','c','d','e'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, DataFrames are essentially collections of Series. As such, it should come as no surprise that DataFrames can be initialized from Series as well. DataFrames initialized from Series, however, obey the indices of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame( dict(foxtrot = s1, yankee = s3, hotel = s1) )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing into and slicing DataFrames is very similar to Series. With DataFrames, elements can be accessed by row (index), column, or both. A column of a DataFrame can be indexed just like it were a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['foxtrot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns also become attributes of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.foxtrot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple columns can be indexed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['foxtrot', 'hotel']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **columns** attribute of a DataFrame can be used to index multiple columns as once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df[ df.columns[::2] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index by row by specifying index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['i1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using slicing syntax with a DataFrame indexes across multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loc** attribute can be used to slice by both column and row. Slicing does not work in the same way here unfortunately. Indices need to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.index[:3], ['foxtrot', 'yankee']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional indexing can also be performed by finding values that meet some criterion. Below we return all rows of the DataFrame where *yankee* is greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['yankee'] > 0.5 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of DataFrames (matrix, columns, indices) are very easily accessible. Moreover, DataFrames are easily converted into other types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DataFrame to NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DataFrame to dictionary of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading/Writing DataFrames\n",
    "Pandas makes it very easy to read and write DataFrames. A number of Pandas functions support reading in different types of data formats, and the DataFrame object has built into it several functions for writing information to disk. \n",
    "\n",
    "We will load in the first gambling dataset of the first subject. (For a description of the dataset, see the **Putting it all together** section below.) The file is tab-separated, so we will specify tab as the appropriate separator. Note that read_table has many, many additional arguments to support reading in data of different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas import read_excel, read_csv, read_table\n",
    "\n",
    "df = read_table(os.path.join('gambling', 'subj01_run01.tsv'), sep='\\t')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## To write a DataFrame to file, check the DataFrame.to_ functions.\n",
    "# df.to_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding and dropping columns is very simple with Pandas. Here we will remove several unnecessary columns (e.g. onset, duration parametric loss, etc.) and then add a new variable, diff, which is the difference of the gain and loss variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop unnecessary variables.\n",
    "drop_columns = ['onset','duration','parametric loss', 'distance from indifference',\n",
    "                'parametric gain', 'PTval']\n",
    "df = df.drop(drop_columns, axis=1)    # axis specifies that we are dropping along columns.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add difference column.\n",
    "df['diff'] = df['gain'] - df['loss']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also easy to rename columns. This can be accomplished by directly saving column names to the DataFrame colummns attribute, or we can use the rename attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'response_time':'rt'})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping rows is similarly easy. In this dataset, any respcat = -1 corresponds to a missing response. Let's remove those from the dataset. This can be achieved in a variety of ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aproach 1: Remove directly by indexing.\n",
    "df[df['respcat'] != -1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 2: Set all undesirable values to NaN. Use dropna.\n",
    "df['respcat'] = np.where(df['respcat'] == -1, np.nan, df['respcat'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notice the index reflects the dropped trials!\n",
    "df = df.dropna()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional useful functions include: **transpose**, **drop_duplicates**, and **fillna.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.transpose;          # Tranpose the DataFrame (rows <--> cols)\n",
    "df.drop_duplicates;    # Remove all duplicate rows from the DataFrame.\n",
    "df.fillna;             # Fill all NaNs in DataFrame with specified value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting a DataFrame is achieved with the **sort_values** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting by one column.\n",
    "df.sort_values('gain').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting by multiple columns.\n",
    "df.sort_values(['gain', 'loss'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames have the same summary attributes as NumPy arrays (e.g. max, min, mean, std, median, mode). These can be applied selectively to specrific columns/rows, across columns/rows, or across the entire DataFrame. The **describe** attribute is especially useful for generating a quick description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **value_counts** attribute of DataFrames replaces np.unique(x, return_counts=True). It's much easier to type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['respnum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important attribute to highlight is **apply**. This function is similar to **apply_across_axis**, and can be used to apply a command row-wise or column-wise across a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply zscore function across columns.\n",
    "def zscore(arr):\n",
    "    return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "df.apply(zscore, axis=0).head(5)    # Apply across columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DataFrames\n",
    "Combining DataFrames is one of the most important operations in Pandas. This is achieved in one of three ways:\n",
    "1. **Append:** add row to existing DataFrame\n",
    "2. **Concat:** join two or more DataFrames with identical columns\n",
    "3. **Merge:** join two or more DataFrames with non-overlapping columns based on a specified index.\n",
    "\n",
    "The **append** attribute is used to store new rows to the DataFrame. It accepts Series or dictionary objects as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make series of ones with length equal to columns.\n",
    "series = Series( np.ones_like(df.columns), index=df.columns )\n",
    "\n",
    "## Append.\n",
    "df.append(series, ignore_index=True).tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make dictionary.\n",
    "d = dict(gain = 1, loss = 1, respnum = 1, respcat = 1, rt = 1, diff = 1)\n",
    "\n",
    "## Append.\n",
    "df.append(d, ignore_index=True).tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **concat** command from pandas can be used to join two DataFrames along their rows. If the DataFrames do not fully share columns, NaNs are used to fill missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import concat\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df2 = concat([ df, df[df.columns[:3]] ])\n",
    "\n",
    "print(df2.shape)\n",
    "df2.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the **merge** command can be used to vertically join two DataFrames along some specified index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate DataFrame into two sets of variables.\n",
    "df1 = df[['gain','loss','respnum','respcat']]\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['gain','loss','rt','diff',]]\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge on columns (gain, loss)\n",
    "df3 = df1.merge(df2, on=['gain','loss'], how='inner')\n",
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge on index (duplicates overlapping columns).\n",
    "df3 = df1.merge(df2, left_index=True, right_index=True)\n",
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "Using the material we've covered so far, we will assemble and describe a dataset.\n",
    "\n",
    "The dataset we will be working with was taken from [Tom et al. (2007): *The Neural Basis of Loss Aversion\n",
    "in Decision-Making Under Risk*](http://science.sciencemag.org/content/315/5811/515.full). In this neuroimaging experiment, the brains of 16 individuals were imaged while they performed a task probing decision making. In the task, participants were presented with 50/50 gambles comprised of potential gains and losses. The gains and losses were independently varied, where gains ranged from \\$10 to \\$40 (in increments of \\$2) and losses ranged from \\$5 to \\$20 (in increments of $1). Participants were instructed to respond with one of four levels of judgments: (1) strongly accept, (2) weakly accept, (3) weakly reject, and (4) strongly reject. Participants each completed 256 total gambles, across three sessions.\n",
    "\n",
    "The behavioral data for this experiment is stored in the *gambling* folder inside the *Module 2* folder.\n",
    "\n",
    "### Load and merge gambling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import DataFrame, concat, read_table\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Specify paths to files. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## os.listdir is a useful command that lists all of the files in\n",
    "## a given directory. We are using the string attribute, startswith,\n",
    "## to limit our search to only files starting with 'subj'.\n",
    "\n",
    "tsv_files = [f for f in os.listdir('gambling') if f.startswith('subj')]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize an empty list for storing loaded dataframes.\n",
    "dataframes = []\n",
    "\n",
    "## Iterate over TSV files.\n",
    "for tsv in tsv_files:\n",
    "    \n",
    "    ## Get subject/run ID: Here we remove the file extension\n",
    "    ## and split the filename string on the underscore. That \n",
    "    ## way we can store the subject ID and run number.\n",
    "    subj, run = tsv.replace('.tsv','').split('_')\n",
    "    \n",
    "    ## Load dataframe. \n",
    "    ## os.path.join creates a filepath with its inputs\n",
    "    ## using the correct path separator for the OS.\n",
    "    df = read_table( os.path.join('gambling', tsv) )\n",
    "    \n",
    "    ## Drop undesired columns.\n",
    "    drop_columns = ['onset','duration','parametric loss', 'distance from indifference',\n",
    "                    'parametric gain', 'PTval']\n",
    "    df = df.drop(drop_columns, axis=1)    # axis specifies that we are dropping along columns.\n",
    "    \n",
    "    ## Remove all missing responses (i.e. respcat == -1).\n",
    "    df = df[ df['respcat'] != -1 ]\n",
    "    \n",
    "    ## Rename response time variable.\n",
    "    df = df.rename(columns={'response_time':'rt'})\n",
    "    \n",
    "    ## Store three new varaibles.\n",
    "    df['diff'] = df['gain'] - df['loss']    # Create difference variable\n",
    "    df['subj'] = subj                       # Store subject ID.\n",
    "    df['run']  = run                        # Store run number.\n",
    "    \n",
    "    ## Append to list.\n",
    "    dataframes.append( df )\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Concatenate dataframes.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Concatenate dataframes.\n",
    "data = concat(dataframes) \n",
    "\n",
    "## Sort dataframe.\n",
    "data = data.sort_values(['subj','run','gain','loss'])\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cleanliness, let's reorder the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['subj','run','gain','loss','diff','respnum','respcat','rt']]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the index is out of order. This makes sense given how we combined and resorted the data. Let's reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save aggregated data.\n",
    "Saving a dataframe is very easy and requires only a single line of code. Here we will save the data as a CSV file, but other options exist including: Excel, HDF, HTML, JSON, SQL, STATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## We set index to False to ignore the indices, as they provide no information. \n",
    "data.to_csv(os.path.join('gambling', 'group_gambling_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data\n",
    "Briefly, it's worth mentioning that Pandas allows for the storage of categorical data. This will map strings to unique integers, and represent the strings as integers internally. This can consierably reduce the memory usage of a DataFrame for large text-heavy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Categorical\n",
    "\n",
    "data['subj'] = Categorical(data['subj'])\n",
    "data['run'] = Categorical(data['run'])\n",
    "\n",
    "data.info(memory_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Tables\n",
    "The groupby command is a simple yet powerful command that allows the user to aggregate, transform, and filter data efficiently. We will provide examples of each in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create groupby object, grouped over subject.\n",
    "gb = data.groupby('subj')\n",
    "gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation\n",
    "Compute some summary statistic over groups in the data. For example, we will compute some averages across subjects here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute mean over responses and reaction time.\n",
    "gb[['respcat','rt']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is possible to create groupby objects over multiple dimensions.\n",
    "gb = data.groupby(['gain','loss'])\n",
    "gb.respcat.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **aggregate** attribute can be used to summarize/aggregate groupby objects with functions not already available as attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gba = gb.aggregate(np.percentile, q=70)\n",
    "gba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loc** attribute can be used into index into aggregated GroupBy objects. Similarly to before, indexing follows the first, second, third..., etc. variable onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve 70th percentile for gain=30, loss=15\n",
    "gba.loc[30,15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, **pivot/pivot_table** functions of DataFrames can accomplish something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use pivot_table to compute the equivalent of GroupBy \n",
    "## on gain & loss, averaging over respcat.\n",
    "data.pivot_table(index='gain', columns='loss', values='respcat', aggfunc='mean').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation\n",
    "Transform values meeting some specific criterion according to a specified function. Here we will impute missing response times as the mean of the response times for a given subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a copy of the original dataset.\n",
    "missing_data = data.copy()\n",
    "\n",
    "## Add missing data for two subjects.\n",
    "d = dict(subj='subj01', run='run01', gain=10, loss=6, diff=4, \n",
    "         respnum=2, respcat=0, rt=np.nan)\n",
    "missing_data = missing_data.append(d, ignore_index=True)\n",
    "\n",
    "d = dict(subj='subj10', run='run01', gain=10, loss=6, diff=4,\n",
    "         respnum=2, respcat=0, rt=np.nan)\n",
    "missing_data = missing_data.append(d, ignore_index=True)\n",
    "\n",
    "missing_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Make groupby object.\n",
    "gb = missing_data.groupby('subj')\n",
    "\n",
    "## Transform by subject on missing data.\n",
    "imputed_data = gb.transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "imputed_data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtration\n",
    "Filter a dataset accordign to some criterion. Here we will create a new subset of data comprised of only subjects who took the risky bet more than half the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by subject.\n",
    "gb = data.groupby('subj')\n",
    "\n",
    "## Filter on responses.\n",
    "filtered_data = gb.filter(lambda x: x.respcat.mean() > 0.5)\n",
    "\n",
    "## Identify remaining subjects.\n",
    "filtered_data['subj'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Visualization\n",
    "## Matplotlib\n",
    "Matplotlib, or the Matlab plotting library, is the core plotting package of the scientific python distribution. The aim of Matplotlib is to recreate all of the plotting capabilities of Matlab in python. As such, much of the syntax/style of Matplotlib reflects Matlab plotting. \n",
    "\n",
    "We will go through the syntax of plotting the five most common types of plots: bar plots, line plots, scatter plots, boxplots, and heatmaps. We will also cover adding details to plots (e.g. axes, titles, legends, errorbars), making multiple plots in one figure, and scaling/sizing plots.\n",
    "\n",
    "Similar to plotting in R, pure Matplotlib is a little clunky and a lot of code is needed to make more visually appealing plots. For this reason, we will introduce the Seaborn package later. Seaborn is similar to ggplot2 in that, with a tidy dataframe and some knowledge of the syntax, beautiful plots can quickly/easily be generated. But, it's better to crawl before walking, so we'll start with Matplotlib first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "## NOTE: The second line is a bit of notebook magic! \n",
    "## It's a jupyter-notebook shortcut that makes all\n",
    "## plots be displayed at the bottom of a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures & Axes\n",
    "A brief note: In Matplotlib jargon, an axis is a plot (e.g. barplot, scatterplot) and a figure is the surrounding object containing all plots. The most basic figure contains a single axis (i.e. one plot). More complex figures may have multiple plots of different sizes and numbers per row. \n",
    "\n",
    "This distinction is important because certain graphical tweaks can only be applied to figures or axes. For example, figures control the size of the canvas, the spacing of plots, and saving figures. Axes control plot-specific features, including labels, titles, and legends. To start, we will only generate figures with one plot. Later, we will introduce drawing multiple plots per figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplots\n",
    "Barplots are probably the least intuitive plot in Matplotlib because the user must specify the starting point and width of the bars (this is in contrast to other languages that automatically assign x-coordinates to the bars). Though clunky, this does provide some additional control to the user. \n",
    "\n",
    "In this example, we will plot the average response within subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "## Make a new axis: The numbers correspond to\n",
    "## row, column, and plot index. For example,\n",
    "## subplot(211) would mean creating the \n",
    "## first plot of a figure with 2 rows and 1 column.\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "## Use groupby to compute average response.\n",
    "avg_resp = data.groupby('subj').respcat.mean()\n",
    "\n",
    "## Now we have to specify the starting positions of\n",
    "## the bars along the x-axis. We will have each bar\n",
    "## begin at a sequential integer for each new subject\n",
    "## (i.e., 0, 1, 2, ... N).\n",
    "n_subj = len(avg_resp)\n",
    "xpos = np.arange(n_subj)\n",
    "\n",
    "## Now we can plot.\n",
    "width = 0.9\n",
    "ax.bar(left=xpos, height=avg_resp, width=width, color='#7ec0ee');\n",
    "\n",
    "## Fix x-axis.\n",
    "ax.set_xlim(-0.1);                                   # Start x-axis at -0.1 to leave equal room \n",
    "                                                     # on both sides.\n",
    "ax.set_xticks(xpos + width / 2.);                    # Set x-tickmarks at center of bars.\n",
    "ax.set_xticklabels(data.subj.unique(), fontsize=12); # Set subjects as x-ticklabels\n",
    "ax.set_xlabel('Subjects', fontsize=18);              # Set x-axis label.\n",
    "\n",
    "## Fix y-axis.\n",
    "ax.set_ylim(0,1);\n",
    "ax.set_ylabel('Average Response', fontsize=18);\n",
    "\n",
    "## Set title.\n",
    "ax.set_title('Example Barplot', fontsize=24);\n",
    "\n",
    "## Autoscale image.\n",
    "plt.tight_layout();    # Reduce whitespace outside of plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped barplots must be manually generated, as in the following example where we plot the average response and reponse time data by subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "## Make a new axis.\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "## Use groupby to compute averages.\n",
    "gb = data.groupby('subj')\n",
    "avg_resp = gb.respcat.mean()\n",
    "std_resp = gb.respcat.std()\n",
    "avg_rt = gb.rt.mean()\n",
    "std_rt = gb.rt.std()\n",
    "\n",
    "## Specify the starting positions of the bars.\n",
    "n_subj = 5\n",
    "resp_pos = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "rt_pos = [1.0, 3.5, 6.0, 8.5, 11.0]\n",
    "\n",
    "## Make barplots. Here we use the yerr flag to make error bars.\n",
    "## we also define the condition using label.\n",
    "width = 1.0\n",
    "ax.bar(resp_pos, avg_resp[:n_subj], width, label = 'Choice',\n",
    "       yerr=std_resp[:n_subj], color='#7ec0ee');\n",
    "ax.bar(rt_pos, avg_rt[:n_subj], width, label='RT',\n",
    "       yerr=std_rt[:n_subj], color='#71eeb8');\n",
    "\n",
    "## Fix x-axis.\n",
    "ax.set_xlim(-0.1, 12.1);\n",
    "ax.set_xticks(rt_pos);\n",
    "ax.set_xticklabels(data.subj.unique(), fontsize=12);\n",
    "ax.set_xlabel('Subjects', fontsize=18);\n",
    "\n",
    "## Set title.\n",
    "ax.set_title('Example Grouped Barplot', fontsize=24);\n",
    "\n",
    "## Set horizontal line. Starts at zero and travels \n",
    "## length of plot.\n",
    "ax.hlines(0, -0.1, 12.1, color='black')\n",
    "\n",
    "## Add legend.\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "## Autoscale image.\n",
    "plt.tight_layout();    # Reduce whitespace outside of plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineplots\n",
    "Lineplots are more intuitive than are barplots, requirng at the minimum only the x- and y-datapoints. Many tweaks and embellishments can similarly be added. \n",
    "\n",
    "Here we will plot the nominal likelihood-of-take of the risky bet against the diff values. In the example, we use the Matplotlib color shorthands. These are:\n",
    "* b: blue \n",
    "* g: green \n",
    "* r: red \n",
    "* c: cyan \n",
    "* m: magenta \n",
    "* y: yellow\n",
    "* k: black \n",
    "* w: white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "## Make a new axis.\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "## Compute groupby gain.\n",
    "gb = data.groupby('diff')\n",
    "respnum_avg = gb.respnum.mean()\n",
    "\n",
    "## Plot average gain response.\n",
    "## In this example, we use the matplotlib color shorthands. \n",
    "ax.plot( respnum_avg.index, respnum_avg, color='b', linewidth=3 );\n",
    "\n",
    "## We will also shade within 1sd of the line. To do this, we use\n",
    "## fill_between, which asks for the x-points and the y-lower/upper\n",
    "## bounds along which to fill. The alpha parameter below reflects\n",
    "## the transparency from {transparent = 0.0, opaque = 1.0}.\n",
    "respnum_std = gb.respnum.std()\n",
    "ax.fill_between( respnum_avg.index, respnum_avg - respnum_std, respnum_avg + respnum_std,\n",
    "                color='b', alpha=0.2)\n",
    "\n",
    "## We will also add dotted lines to demarcate the bounds of +- 1sd. \n",
    "ax.plot( respnum_avg.index, respnum_avg - respnum_std, linewidth=0.5, linestyle='--', color='k' )\n",
    "ax.plot( respnum_avg.index, respnum_avg + respnum_std, linewidth=0.5, linestyle='--', color='k' )\n",
    "\n",
    "## Add details.\n",
    "ax.set_xlabel('Gain - Loss', fontsize=18)\n",
    "ax.set_yticks([1,2,3,4])\n",
    "ax.set_yticklabels(['Strong Accept', 'Weak Accept', 'Weak Reject', 'Strongly Reject'])\n",
    "ax.tick_params(axis='both',which='major',labelsize=14)\n",
    "ax.set_title('Example Line Plot', fontsize=24)\n",
    "\n",
    "## Autoscore.\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots\n",
    "The synxtax of scatterplots is similar to that of lineplots. Whereas lineplots have different [linestyles](https://matplotlib.org/examples/lines_bars_and_markers/line_styles_reference.html), scatterplots have different [marker styles](https://matplotlib.org/api/markers_api.html). \n",
    "\n",
    "Here we make scatterplots of the average reaction time by diff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "## Make a new axis.\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "## Plot gain. \"s\" controls the size of the marker; marker control the shape.\n",
    "## Edgecolor adds an outline to the marker.\n",
    "rt = data.groupby(['diff']).rt.mean()\n",
    "ax.scatter( rt.index, rt, s=100, marker='o', color='c', edgecolor='k');\n",
    "\n",
    "## Add details.\n",
    "ax.set_xlim(-11, 36)\n",
    "ax.set_xlabel('Gain - Loss', fontsize=18)\n",
    "ax.set_ylabel('Reaction Time (s)', fontsize=18)\n",
    "ax.tick_params(axis='both',which='major',labelsize=14)\n",
    "ax.set_title('Example Scatter Plot', fontsize=24)\n",
    "\n",
    "## Autoscore.\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histrograms\n",
    "Histograms are very easy fortunately. Here we will plot two subjects reaction times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "## Make a new axis.\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "## RT distribution for subj06.\n",
    "ax.hist( data.loc[data.subj == 'subj06', 'rt'], bins=25, \n",
    "        label='subj06', color='#1b9e77', alpha=0.75 );\n",
    "\n",
    "## RT distribution for subj10.\n",
    "ax.hist( data.loc[data.subj == 'subj10', 'rt'], bins=25, \n",
    "        label='subj10', color='#7570b3', alpha=0.75 );\n",
    "\n",
    "## Add details.\n",
    "ax.set_xlabel('Reaction Time (s)', fontsize=18)\n",
    "ax.set_ylabel('Frequency', fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_title('Example Histogram', fontsize=24)\n",
    "ax.legend(loc='best', fontsize=16, frameon=False);\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps\n",
    "Heatmaps are very useful plots, but slightly counterintuitive in Matplotlib. We will go through an example looking at the average likelihood-of-take as a function of gain and loss. A full list of colormaps can be found [here](https://matplotlib.org/examples/color/colormaps_reference.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "## Make a new axis.\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "## Make groupby object by gain and loss.\n",
    "resp = data.groupby(['gain','loss']).respcat.mean()\n",
    "\n",
    "## Extract as matrix and reshape (there are 16 unique values for each).\n",
    "resp = resp.as_matrix().reshape(16,16).T\n",
    "\n",
    "## Plot. The parameters are as follows:\n",
    "#### aspect: defines scaling. automatic scaling is preferable.\n",
    "#### interpolation: smoothing of image. We want no smoothing.\n",
    "#### origin: upper or lower, we want smaller values to begin in lower corner.\n",
    "#### cmap: what colormap to use.\n",
    "#### vmin, vmax: min,max values of colormap.\n",
    "cbar = ax.imshow(resp, aspect='auto', interpolation='none', \n",
    "                 origin='lower', cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "## Add details.\n",
    "ax.set_xticks(np.arange(0,16,2))\n",
    "ax.set_xticklabels(np.unique(data.gain)[::2], fontsize=14)\n",
    "ax.set_xlabel('Gain', fontsize=24)\n",
    "\n",
    "ax.set_yticks(np.arange(0,16,2))\n",
    "ax.set_yticklabels(np.unique(data.loss)[::2], fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=24)\n",
    "\n",
    "## Add colorbar.\n",
    "cbar = plt.colorbar(cbar, ax=ax);\n",
    "cbar.ax.tick_params(labelsize=14) \n",
    "cbar.set_label('Average Response', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Multiple Plots in a Figure\n",
    "With Matplotlib, there are 3.5 methods for constructing a figure with multiple embedded plots. These vary from minimal control of layout to maximal control of layout, and are as follows:\n",
    "1. subplot/subplots: generates equal sized plots in a figure.\n",
    "2. subplot2grid: generates plots of different sizes along a grid, minimal spacing options.\n",
    "3. gridspec: generates plots of different sizes, many spacing options.\n",
    "\n",
    "We have previously present subplot. Briefly we will show how subplot and subplots can be used to make a multiply-embedded figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Subplot example: Figure needs to be called.\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "## Make 2x2 figure.\n",
    "ax = plt.subplot(2,2,1)\n",
    "ax.text(0.5,0.5,'ax1', fontsize=18, ha='center', va='center');\n",
    "\n",
    "ax = plt.subplot(2,2,2)\n",
    "ax.text(0.5,0.5,'ax2', fontsize=18, ha='center', va='center');\n",
    "\n",
    "ax = plt.subplot(2,2,3)\n",
    "ax.text(0.5,0.5,'ax3', fontsize=18, ha='center', va='center');\n",
    "\n",
    "ax = plt.subplot(2,2,4)\n",
    "ax.text(0.5,0.5,'ax4', fontsize=18, ha='center', va='center');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Subplots example: Figure can be called directly from command.\n",
    "\n",
    "## Make 2x2 figure. Note that figsize can be directly called.\n",
    "## Axes is a [2,2]-list of axes.\n",
    "fig, axes = plt.subplots(2,2,figsize=(5,5))\n",
    "\n",
    "## Iteratively add text.\n",
    "for n in range(4):\n",
    "    axes[n/2,n%2].text(0.5,0.5,'ax%s' %(n+1), fontsize=18, ha='center', va='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use subplot2grid(), you provide geometry of the grid and the location of the subplot in the grid. Here we present an example geometry for a 3x3 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## subplot2grid example: Figure must be called.\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "## Call subplot2grid. First argument specifies the global\n",
    "## layout of the figure. Second argument specifies which\n",
    "## axis you are generating. colspan/rowspan describes the\n",
    "## length of the axes within the grid of the figure.\n",
    "\n",
    "ax = plt.subplot2grid((3, 3), (0, 0), colspan=3)\n",
    "ax.text(0.5,0.5,'ax1', fontsize=18, ha='center', va='center');\n",
    "\n",
    "ax = plt.subplot2grid((3, 3), (1, 0), colspan=2)\n",
    "ax.text(0.5,0.5,'ax2', fontsize=18, ha='center', va='center');\n",
    "\n",
    "ax = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n",
    "ax.text(0.5,0.5,'ax3', fontsize=18, ha='center', va='center');\n",
    "\n",
    "ax = plt.subplot2grid((3, 3), (2, 0))\n",
    "ax.text(0.5,0.5,'ax4', fontsize=18, ha='center', va='center');\n",
    "\n",
    "ax = plt.subplot2grid((3, 3), (2, 1))\n",
    "ax.text(0.5,0.5,'ax5', fontsize=18, ha='center', va='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridspec objects are similar to subplot2grid in that they allow different sized plots within a figure. Gridspec objects also allow spacing configuration of axes within the figure. To give an example, we embed two sets of six plots with a large gap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "## Initialize figure.\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "## Define first 3x3 grid. \n",
    "gs = gridspec.GridSpec(3, 3)\n",
    "\n",
    "## Update spacing parameters such that the figures can only\n",
    "## extend to the 0.45 fraction of the figure.\n",
    "gs.update(left=0.05, right=0.45, wspace=0.05)\n",
    "\n",
    "## Create plots by indexing into grid.\n",
    "ax1 = plt.subplot(gs[0, :])\n",
    "ax2 = plt.subplot(gs[1, :-1])\n",
    "ax3 = plt.subplot(gs[1:, -1])\n",
    "ax4 = plt.subplot(gs[-1, 0])\n",
    "ax5 = plt.subplot(gs[-1, -2])\n",
    "\n",
    "## Define second 3x3 grid. \n",
    "gs = gridspec.GridSpec(3, 3)\n",
    "\n",
    "## Update spacing parameters such that the figures can only\n",
    "## start at 0.55 fraction of the figure.\n",
    "gs.update(left=0.55, right=0.95, wspace=0.05)\n",
    "\n",
    "## Create plots by indexing into grid.\n",
    "ax1 = plt.subplot(gs[0, :])\n",
    "ax2 = plt.subplot(gs[1, :-1])\n",
    "ax3 = plt.subplot(gs[1:, -1])\n",
    "ax4 = plt.subplot(gs[-1, 0])\n",
    "ax5 = plt.subplot(gs[-1, -2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also includes a number of plotting functions that help to cut down on code. For example, barplots can be constructed directly from a DataFrame object. The trouble is that these plots are fairly limited and require similar manual tweaking to get more aesthetically pleasing. They are useful, however, for quick inspections of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Use groupby to compute averages.\n",
    "gb = data.groupby('subj')\n",
    "avg_resp = gb.respcat.mean()\n",
    "\n",
    "## Make barplot from groupby object.\n",
    "avg_resp.plot(kind='bar', figsize=(12,4), title='Example Pandas Barplot', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Compute groupby gain.\n",
    "gb = data.groupby('diff')\n",
    "respnum_avg = gb.respnum.mean()\n",
    "\n",
    "## Plot lineplot.\n",
    "respnum_avg.plot('line', linewidth=3, figsize=(12,4), title='Example Pandas Lineplot', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn\n",
    "The Seaborn statistical data visualization library was created to be the equivalent of ggplot2 for python. In other words, it is designed to rapidly turn around publication-ready plots from Pandas DataFrames with as minimal code as necessary. The [documention](https://seaborn.pydata.org/) is full of great examples that should be checked out. We will go through a few examples here.\n",
    "\n",
    "### Style and Context\n",
    "One of the great things about Seaborn is setting defaults. The defaults set a variety of parameters (e.g. colors, fonts, font sizes, etc.) that result in little tweaking of figures down the line. We introduce those two functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "## set_style sets the aesthetic style of the plots. This most dramatically \n",
    "## affects the background of plots and the presence (or absence) of gridlines.\n",
    "sns.set_style('whitegrid')      # {white, whitegrid, dark, darkgrid}\n",
    "\n",
    "## set_context sets the context parameters, affecting the size of labels,\n",
    "## lines, and other elements of the plot.\n",
    "sns.set_context('poster') # {notebook, paper, talk, poster}\n",
    "\n",
    "## Text plot.\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(10), np.arange(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplots\n",
    "Let's first start by recreating the barplot from earlier (i.e. average response within subjects). As can be seen, substantially fewer lines of code are necessary. Moreover, 95% CIs are computed via bootstrap resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize figure.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "## Plot: Provide the x-axis, the y=axis, and dataframe.\n",
    "sns.barplot('subj', 'respcat', data=data, color='#7ec0ee');\n",
    "\n",
    "## Scale and cleanup plot.\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "## Plot line plot.\n",
    "ax = sns.regplot('diff', 'respnum', data=data, x_bins=np.unique(data['diff']));\n",
    "\n",
    "## Fix axes.\n",
    "ax.set_xlim(-11,36)\n",
    "ax.set_ylim(0.5,4.5)\n",
    "ax.set_yticks([1,2,3,4])\n",
    "ax.set_yticklabels(['Strong Accept', 'Weak Accept', 'Weak Reject', 'Strongly Reject'])\n",
    "\n",
    "## Scale and cleanup plot.\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "## Plot line plot.\n",
    "ax = sns.regplot('diff', 'respcat', data=data, x_bins=np.unique(data['diff']), logistic=True);\n",
    "\n",
    "## Fix axes.\n",
    "ax.set_xlim(-11,36)\n",
    "ax.set_ylim(-0.05,1.05)\n",
    "\n",
    "## Scale and cleanup plot.\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open new figure canvas. Define its size.\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "## Make scatterplot.\n",
    "ax = sns.regplot('diff', 'rt', data=data, order=2, x_bins=np.unique(data['diff']));\n",
    "ax.set_xlim(-11,36)\n",
    "\n",
    "## Scale and cleanup plot.\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data, col='subj', col_wrap=4, sharex=True, sharey=False)\n",
    "g.map(plt.hist, 'rt', bins=15, color='#1b9e77', lw=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot = data.pivot_table(index='loss', columns='gain', values='respcat')\n",
    "ax = sns.heatmap(pivot, vmin=0, vmax=1, cmap='Blues', linewidths=.5)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics (SciPy + Statsmodels)\n",
    "Statsmodels is the prominent statistical models package in the scientific python distribution. Statsmodels provides functionality for linear regression, generalized linear models, limited dependent variable models, ARMA and VAR models. The [Statsmodels documentation](http://www.statsmodels.org/stable/index.html) provides a full list of models and functions implemented. It draws its inspiration from the most popular R statistics packages (e.g. lme4) and uses the same statistical modeling syntax as R (e.g. \"y ~ x\"). As we will see, the package is still new and relatively limited as of the time of writing. Though the most basic models are implemented, more complex yet standard models (e.g. mixed-effects logistic regression) are not yet implemented. \n",
    "\n",
    "## Scipy Statistics Module\n",
    "Before covering Statsmodels, we will first briefly introduce the scientific python (scipy) package. Expanding from NumPy (which provides the backend of arrays, matrices, and array-based functions), SciPy introduces a series of special modules for different important computations, including: integration, optimization, signal processing, image processing, and statistics. The [SciPy documentation](https://docs.scipy.org/doc/scipy/reference/) details the many powerful tools the package introduces.\n",
    "\n",
    "The [SciPy stats module](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) (scipy.stats) introduces a number of helpful functions, including:\n",
    "* Statistical distributions (e.g. normal, student-t, inv-normal, gamma, beta, binomal..., [full list](https://docs.scipy.org/doc/scipy/reference/stats.html))\n",
    "* Measures of distributional shape (e.g. kurtosis, skew, QQ-plots, KS-test)\n",
    "* Basic statistical tests (e.g. linear correlation, nonparametric correlation, t-tests, one-way ANOVA, Chi-Square)\n",
    "\n",
    "The SciPy package can be especially helpful when the user needs to compute quick statistics without necessarily needing to implement even simple models. \n",
    "\n",
    "### Interactive Probability Distributions\n",
    "The SciPy package has already implemented a number of standard statistical distributions beyond the normal distribution. This can be combined with the awesome ipywidgets module to make interactive plots that help in understanding the influence of parameters in statistical distributions. Below we highlight the inverse-gamma distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import invgamma\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_invgamma(alpha, beta, n_samp=1000):\n",
    "    \n",
    "    ## Sample from inverse-gamma distribution.\n",
    "    np.random.seed(47404)\n",
    "    arr = invgamma(alpha,loc=0,scale=beta).rvs(n_samp)\n",
    "    arr = arr[arr < 12] # prune for visualization purposes\n",
    "    \n",
    "    ## Plot.\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.hist(arr, bins=25);\n",
    "    plt.xlim(0,12);\n",
    "    sns.despine();\n",
    "    \n",
    "interact(plot_invgamma, alpha=(0.1,5,0.1), beta=(0.1,5,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "Scipy has a number of functions useful for diagnosing the shape of data distributions, including higher order moments (kurtosis, skew, etc.). We highlight these functions with the case of reaction times, which tend to be heavily right-skewed (heavy tails to the right of the central tendency). We measure the kurtosis of Subject 1's reaction times, and plot a QQ-plot fitted to a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, probplot\n",
    "\n",
    "## Extract subj01 RTs.\n",
    "rt = data.loc[data.subj == 'subj01', 'rt']\n",
    "\n",
    "## Compute skew.\n",
    "computed_skew = kurtosis(rt, fisher=True)\n",
    "print('The measured kurtosis is %0.3f. (Normal: kurt=0)' %computed_skew)\n",
    "\n",
    "## QQ-plot.\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "probplot(rt, plot=ax);\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics\n",
    "SciPy also has implemented several basic statistical functions for quickly computing statistics:\n",
    "* Correlations: Pearson correlation (pearsonr), Spearman correlation (spearmanr), Kendall Tau (kendalltau)\n",
    "* T-tests: one-sample t-test (ttest_1samp), two-sample t-test (ttest_ind), dependent sample t-test (ttest_rel)\n",
    "* ANOVA: One-way ANOVA (f_oneway)\n",
    "* Chi-square (chisquare, chi2_contingency)\n",
    "\n",
    "We will quickly highlight a few of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind, f_oneway\n",
    "\n",
    "## Simulate data from multivariate normal distribution.\n",
    "## Two variables will share mean = 2, and two variables\n",
    "## will be 0.4 correlated.\n",
    "\n",
    "mu = [0,2,2]\n",
    "cov = [[1.0, 0.4, 0.0],\n",
    "       [0.4, 1.0, 0.0],\n",
    "       [0.0, 0.0, 1.0]]\n",
    "\n",
    "## Randomly sample 50 observations.\n",
    "np.random.seed(47404)\n",
    "mat = np.random.multivariate_normal(mu, cov, 50)\n",
    "\n",
    "## Transpose so shape=(3,50)\n",
    "mat = mat.T\n",
    "\n",
    "## Compute correlation of pairs: (1,2), (1,3)\n",
    "print('Correlation [1,2]: r = %0.3f, p = %0.3f' %pearsonr(mat[0], mat[1]))\n",
    "print('Correlation [1,3]: r = %0.3f, p = %0.3f' %pearsonr(mat[0], mat[2]))\n",
    "\n",
    "## Compute t-tests of pairs: (1,2), (2,3)\n",
    "print('Indenpendent t-test [1,2]: t = %0.3f, p = %0.3f' %ttest_ind(mat[0], mat[1]))\n",
    "print('Indenpendent t-test [2,3]: t = %0.3f, p = %0.3f' %ttest_ind(mat[1], mat[2]))\n",
    "\n",
    "## Compute one-way ANOVA.\n",
    "print('Oneway ANOVA: f = %0.3f, p = %0.3f' %f_oneway(*mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels\n",
    "If you are familiar with R-styled formulas for regression, then Statsmodels + Pandas is a very powerful combo of packages for data analysis in python. We will cover only a few select examples, but know that many models are already implemented (e.g. OLS, GLM, GEE, WLS). Many more well-documented tutorials can be found [here](http://www.statsmodels.org/stable/examples/index.html#notebook-examples) and [here](https://github.com/statsmodels/statsmodels/wiki/Examples). \n",
    "\n",
    "### Linear Regression (OLS)\n",
    "Below is a basic ordinary least squares (OLS) linear regression model measuring the relationship of subjective likelihood-of-take (respnum) against gain and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "\n",
    "## Define formula.\n",
    "formula = 'respnum ~ gain + loss'\n",
    "\n",
    "## Define model.\n",
    "model = OLS.from_formula(formula, data=data)\n",
    "\n",
    "## Fit model.\n",
    "result = model.fit()\n",
    "\n",
    "## Print summary.\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see if using only the diff variable is a more parsimonious fit of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define formula.\n",
    "formula = 'respnum ~ diff'\n",
    "\n",
    "## Define model.\n",
    "model = OLS.from_formula(formula, data=data)\n",
    "\n",
    "## Fit model.\n",
    "result = model.fit()\n",
    "\n",
    "## Print summary.\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear mixed effects models\n",
    "We might want to try a mixed effects model here, assuming that some people have trait-level decisional biases (e.g. risk-averse, risk-seeking). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import MixedLM\n",
    "\n",
    "## Define formula.\n",
    "formula = 'respnum ~ gain + loss'\n",
    "\n",
    "## Define model.\n",
    "model = MixedLM.from_formula(formula, data=data, groups=data.subj)\n",
    "\n",
    "## Fit model.\n",
    "result = model.fit()\n",
    "\n",
    "## Print summary.\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Print random effects.\n",
    "re = result.random_effects\n",
    "for k,v in sorted(re.iteritems()):\n",
    "    print('%s = %0.3f' %(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Below is a basic logistic regression model measuring the relationship of categorical likelihood-of-take (respcat) against gain and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import Logit\n",
    "\n",
    "## Define formula.\n",
    "formula = 'respcat ~ gain + loss'\n",
    "\n",
    "## Define model.\n",
    "model = Logit.from_formula(formula, data=data)\n",
    "\n",
    "## Fit model.\n",
    "result = model.fit()\n",
    "\n",
    "## Print summary.\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA\n",
    "As a basic example, we will perform a simple ANOVA testing for differences in the average reaction time across the diferent subjective likelihood-of-take values (respnum). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "## Aggregate reaction times by subject and response category.\n",
    "pivot_table = data.pivot_table(index='subj', columns='respnum', values='rt', aggfunc='median')\n",
    "\n",
    "## Melt pivot table to long list.\n",
    "llpt = pivot_table.melt(value_name='rt')\n",
    "\n",
    "## Make respnum a categorical variable.\n",
    "llpt['respnum'] = Categorical(llpt['respnum'])\n",
    "\n",
    "## Define formula.\n",
    "formula = 'rt ~ respnum'\n",
    "\n",
    "## Define and fit model.\n",
    "model = OLS.from_formula(formula, data=llpt)\n",
    "fit = model.fit()\n",
    "\n",
    "## Fit ANOVA model.\n",
    "aov_table = sm.stats.anova_lm(fit, typ=2)\n",
    "print aov_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models (GLM)\n",
    "We finish with a generalized linear model showing the improvement of modeling reaction times with the gamma family of distributions. We show a regression of the diff variable and diff-squared on reaction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import GLM, families\n",
    "\n",
    "## Add new column.\n",
    "data['diff2'] = data['diff'] ** 2\n",
    "\n",
    "## Define formula.\n",
    "formula = 'rt ~ diff + diff2'\n",
    "\n",
    "## Fit model (when no family specified, defaults to normal distribution.)\n",
    "result = GLM.from_formula(formula, data=data, subset=data.subj == 'subj01').fit()\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fit model with gamma family.\n",
    "result = GLM.from_formula(formula, data=data, subset=data.subj == 'subj01', \n",
    "                          family=families.Gamma()).fit()\n",
    "result.summary2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
